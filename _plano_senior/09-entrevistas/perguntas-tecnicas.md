# ðŸ§  Perguntas TÃ©cnicas - Entrevistas de Testes de Software

## Ãndice

1. [Objetivo](#1-objetivo)
2. [Como Usar Este Documento](#2-como-usar-este-documento)
3. [Fundamentos de Testes](#3-fundamentos-de-testes)
4. [Qualidade e MÃ©tricas](#4-qualidade-e-mÃ©tricas)
5. [ResiliÃªncia e Confiabilidade](#5-resiliÃªncia-e-confiabilidade)
6. [Performance e Escalabilidade](#6-performance-e-escalabilidade)
7. [Observabilidade](#7-observabilidade)
8. [SeguranÃ§a](#8-seguranÃ§a)
9. [Arquitetura e Design](#9-arquitetura-e-design)
10. [Processos e Cultura](#10-processos-e-cultura)
11. [Trade-offs e DecisÃµes](#11-trade-offs-e-decisÃµes)

---

## 1. Objetivo

Este documento contÃ©m **50+ perguntas reflexivas** para:

- **Entrevistas tÃ©cnicas** (pleno a sÃªnior)
- **AutoavaliaÃ§Ã£o** de conhecimento
- **DiscussÃµes de time** sobre prÃ¡ticas de teste
- **PreparaÃ§Ã£o para promoÃ§Ã£o** a nÃ­veis seniores

**CaracterÃ­sticas das perguntas:**

- âœ… Abertas (nÃ£o hÃ¡ resposta Ãºnica "certa")
- âœ… Avaliam raciocÃ­nio crÃ­tico
- âœ… Exploram trade-offs e contexto
- âœ… Conectam teoria e prÃ¡tica
- âœ… Focam em ferramentas open source

---

## 2. Como Usar Este Documento

### Para Entrevistadores

**Estrutura da Entrevista:**

1. Escolher 3-5 perguntas de diferentes categorias
2. Dar 5-10 minutos para resposta inicial
3. Fazer perguntas de aprofundamento
4. Avaliar usando rubrica (docs/09-entrevistas/rubrica-avaliacao.md)

**Perguntas de Follow-up:**

- "Pode dar um exemplo real onde isso aconteceu?"
- "Quais foram os trade-offs dessa decisÃ£o?"
- "Como vocÃª mediria o sucesso dessa abordagem?"
- "E se o contexto fosse [X] em vez de [Y]?"

### Para Candidatos

**Como Estruturar Respostas:**

1. **Contexto**: Descrever cenÃ¡rio real
2. **Problema**: Qual desafio enfrentou
3. **SoluÃ§Ã£o**: Como resolveu (ferramentas, tÃ©cnicas)
4. **Resultado**: MÃ©tricas de impacto
5. **Aprendizado**: O que faria diferente

**Red Flags a Evitar:**

- âŒ Respostas dogmÃ¡ticas ("sempre use X")
- âŒ Sem considerar contexto
- âŒ Focar apenas em teoria sem prÃ¡tica
- âŒ NÃ£o mencionar trade-offs
- âŒ Ignorar mÃ©tricas/evidÃªncias

---

## 3. Fundamentos de Testes

### ðŸŽ¯ NÃ­vel: Pleno â†’ SÃªnior

#### P1: Unit Tests vs Integration Tests

**Pergunta:**

> VocÃª estÃ¡ revisando um PR onde um colega escreveu testes de integraÃ§Ã£o para toda a lÃ³gica de negÃ³cio, argumentando que "testes de integraÃ§Ã£o sÃ£o mais confiÃ¡veis porque testam o fluxo completo". Como vocÃª responderia? Quando integration tests sÃ£o preferÃ­veis a unit tests?

**O que avaliar:**

- Entendimento da pirÃ¢mide de testes
- Trade-offs (velocidade vs confianÃ§a)
- Capacidade de dar feedback construtivo
- Conhecimento de quando cada tipo Ã© apropriado

---

#### P2: Test Doubles - Mock vs Stub vs Fake

**Pergunta:**

> Explique a diferenÃ§a entre Mock, Stub e Fake. DÃª um exemplo real onde vocÃª escolheu um tipo especÃ­fico de test double e por quÃª. Que problemas vocÃª encontraria se usasse o tipo errado?

**O que avaliar:**

- PrecisÃ£o conceitual (nÃ£o confundir os tipos)
- ExperiÃªncia prÃ¡tica com bibliotecas (Mockito, etc.)
- Entendimento de trade-offs
- Capacidade de identificar anti-patterns (over-mocking)

---

#### P3: Flaky Tests

**Pergunta:**

> Seu time tem 5% de taxa de flakiness. Os testes passam localmente mas falham na CI 1 em cada 20 execuÃ§Ãµes. Como vocÃª investigaria e resolveria isso? Que estratÃ©gias preventivas implementaria?

**O que avaliar:**

- Conhecimento de causas comuns (timing, state, randomness)
- Uso de ferramentas (rerun policies, detectores de flaky)
- Abordagem sistemÃ¡tica de debug
- VisÃ£o de longo prazo (prevenÃ§Ã£o, cultura)

---

#### P4: Test Data Management

**Pergunta:**

> VocÃª precisa testar um fluxo complexo envolvendo 5 entidades relacionadas (User, Order, Payment, Product, Shipping). Como vocÃª gerenciaria a criaÃ§Ã£o desses dados de teste? Que padrÃµes usaria?

**O que avaliar:**

- Conhecimento de Test Data Builders / Object Mother
- Capacidade de manter testes legÃ­veis
- Trade-off entre reutilizaÃ§Ã£o e clareza
- Ferramentas prÃ¡ticas (fixtures, factories)

---

#### P5: AAA vs Given-When-Then

**Pergunta:**

> Seu time debate entre usar AAA (Arrange-Act-Assert) ou Given-When-Then. HÃ¡ alguma diferenÃ§a real? Em que contextos cada abordagem Ã© mais apropriada?

**O que avaliar:**

- Entendimento de que sÃ£o equivalentes estruturalmente
- Contexto importa (BDD â†’ GWT, unit tests â†’ AAA)
- ConsistÃªncia dentro do projeto
- Pragmatismo vs dogmatismo

---

## 4. Qualidade e MÃ©tricas

### ðŸŽ¯ NÃ­vel: Pleno â†’ SÃªnior

#### P6: Code Coverage

**Pergunta:**

> Seu gerente quer aumentar a cobertura de cÃ³digo de 60% para 90%. VocÃª concorda? Por que sim ou nÃ£o? Que mÃ©tricas alternativas ou complementares vocÃª proporia?

**O que avaliar:**

- Entendimento de limitaÃ§Ãµes de coverage
- Conhecimento de mÃ©tricas melhores (mutation score, diff coverage)
- Capacidade de negociar com stakeholders
- VisÃ£o holÃ­stica de qualidade

---

#### P7: Mutation Testing

**Pergunta:**

> VocÃª implementou mutation testing (PITest, Stryker) e descobriu que apesar de 85% de cobertura, o mutation score Ã© apenas 45%. O que isso significa? Como vocÃª melhoraria?

**O que avaliar:**

- CompreensÃ£o profunda de mutation testing
- IdentificaÃ§Ã£o de testes superficiais
- EstratÃ©gia de melhoria (priorizaÃ§Ã£o)
- ExperiÃªncia com ferramentas open source (PITest)

---

#### P8: Diff Coverage

**Pergunta:**

> Seu time debate implementar diff coverage obrigatÃ³rio (80% para cÃ³digo novo). Quais os benefÃ­cios e riscos? Como vocÃª implementaria isso tecnicamente?

**O que avaliar:**

- Conhecimento de ferramentas (Codecov, Coveralls)
- Trade-offs (encorajar qualidade vs overhead)
- IntegraÃ§Ã£o com CI/CD
- ExceÃ§Ãµes e pragmatismo

---

#### P9: Quality Gates

**Pergunta:**

> Projete um sistema de quality gates para um projeto Java/Spring Boot. Quais verificaÃ§Ãµes vocÃª incluiria em cada estÃ¡gio (pre-commit, PR, staging, production)?

**O que avaliar:**

- Conhecimento de mÃºltiplas mÃ©tricas
- EstratÃ©gia em camadas (shift-left)
- Ferramentas open source (SonarQube, etc.)
- Balanceamento velocidade vs qualidade

---

#### P10: Test Pyramid

**Pergunta:**

> VocÃª herdou um projeto com a proporÃ§Ã£o inversa da pirÃ¢mide de testes: 10% unit, 20% integration, 70% E2E. Qual estratÃ©gia vocÃª usaria para inverter isso? Por onde comeÃ§ar?

**O que avaliar:**

- Abordagem incremental e pragmÃ¡tica
- PriorizaÃ§Ã£o (risco vs esforÃ§o)
- RefatoraÃ§Ã£o segura
- MÃ©tricas para medir progresso

---

## 5. ResiliÃªncia e Confiabilidade

### ðŸŽ¯ NÃ­vel: SÃªnior

#### P11: Circuit Breaker

**Pergunta:**

> VocÃª implementou um Circuit Breaker (Resilience4j) para chamadas a um serviÃ§o de pagamento. Como vocÃª testaria os 3 estados (Closed, Open, Half-Open)? Que mÃ©tricas vocÃª monitoraria em produÃ§Ã£o?

**O que avaliar:**

- Entendimento profundo do padrÃ£o
- Testes de diferentes estados
- Observabilidade (mÃ©tricas, alertas)
- ExperiÃªncia com Resilience4j

---

#### P12: Retry com Exponential Backoff

**Pergunta:**

> Quando retry Ã© apropriado e quando Ã© perigoso? DÃª exemplos de operaÃ§Ãµes que devem e nÃ£o devem ter retry. Como vocÃª testaria uma polÃ­tica de retry exponencial?

**O que avaliar:**

- Conhecimento de idempotÃªncia
- Casos de uso (transient failures)
- Anti-patterns (retry storm)
- Testes determinÃ­sticos (Clock mockado)

---

#### P13: IdempotÃªncia

**Pergunta:**

> Explique por que idempotÃªncia Ã© crÃ­tica para resiliÃªncia. Como vocÃª garantiria que um endpoint de pagamento seja idempotente? Como testaria isso?

**O que avaliar:**

- CompreensÃ£o profunda de idempotÃªncia
- TÃ©cnicas (idempotency keys, deduplicaÃ§Ã£o)
- Testes de retry/duplicaÃ§Ã£o
- ImplicaÃ§Ãµes arquiteturais

---

#### P14: Chaos Engineering

**Pergunta:**

> Seu time quer comeÃ§ar com Chaos Engineering. Por onde vocÃª comeÃ§aria? Que experimentos iniciais proporia? Quais ferramentas open source usaria?

**O que avaliar:**

- Conhecimento de Chaos Toolkit, Pumba, Toxiproxy
- Abordagem progressiva (game days â†’ automaÃ§Ã£o)
- HipÃ³teses testÃ¡veis
- Cultura de aprendizado (blameless postmortems)

---

#### P15: Timeout Strategies

**Pergunta:**

> Como vocÃª determinaria o timeout apropriado para chamadas HTTP entre serviÃ§os? Que dados vocÃª coletaria? Como testaria diferentes cenÃ¡rios de timeout?

**O que avaliar:**

- AnÃ¡lise de percentis (p95, p99)
- Testes de latÃªncia/timeout
- Trade-offs (user experience vs resiliÃªncia)
- Ferramentas (Wiremock para simular delays)

---

## 6. Performance e Escalabilidade

### ðŸŽ¯ NÃ­vel: SÃªnior

#### P16: Load vs Stress vs Spike Testing

**Pergunta:**

> Explique a diferenÃ§a entre Load, Stress e Spike testing. Para um e-commerce esperando Black Friday, qual tipo vocÃª priorizaria e por quÃª?

**O que avaliar:**

- Clareza conceitual
- Contexto de negÃ³cio
- Ferramentas (JMeter, Gatling, k6)
- MÃ©tricas relevantes (throughput, latency, error rate)

---

#### P17: Performance Budgets

**Pergunta:**

> VocÃª precisa definir performance budgets para uma API REST. Quais mÃ©tricas usaria? Como testaria continuamente que os budgets nÃ£o sÃ£o violados?

**O que avaliar:**

- MÃ©tricas (p95 latency, throughput, error rate)
- IntegraÃ§Ã£o com CI/CD
- Ferramentas (k6 thresholds, Gatling assertions)
- Quality gates de performance

---

#### P18: N+1 Query Problem

**Pergunta:**

> Como vocÃª detectaria e testaria N+1 query problems? Que ferramentas usaria? Como preveniria regressÃµes?

**O que avaliar:**

- Conhecimento de ORM (JPA, Hibernate)
- Ferramentas (Hibernate statistics, query logs)
- Testes de performance
- Code review practices

---

#### P19: Cache Testing

**Pergunta:**

> VocÃª implementou cache (Redis) para reduzir latÃªncia. Como testaria: 1) TTL correto, 2) cache invalidation, 3) consistÃªncia com DB? Que mÃ©tricas monitoraria?

**O que avaliar:**

- EstratÃ©gias de teste (Testcontainers Redis)
- Edge cases (expiration, eviction)
- MÃ©tricas (hit rate, miss rate)
- Trade-offs (consistÃªncia eventual)

---

#### P20: Database Performance

**Pergunta:**

> Seu teste de carga revela que o banco de dados Ã© o gargalo. Como vocÃª investigaria? Que otimizaÃ§Ãµes consideraria? Como validaria o impacto?

**O que avaliar:**

- AnÃ¡lise de query plans
- Ãndices, particionamento, connection pool
- A/B testing de otimizaÃ§Ãµes
- Ferramentas (pg_stat_statements, EXPLAIN ANALYZE)

---

## 7. Observabilidade

### ðŸŽ¯ NÃ­vel: SÃªnior

#### P21: Logs, MÃ©tricas, Traces

**Pergunta:**

> Explique a diferenÃ§a entre logs, mÃ©tricas e traces. Para debugar um problema de latÃªncia intermitente, qual vocÃª usaria primeiro e por quÃª?

**O que avaliar:**

- Clareza conceitual (3 pilares)
- Casos de uso apropriados
- Ferramentas open source (Prometheus, Jaeger, ELK)
- EstratÃ©gia de troubleshooting

---

#### P22: Distributed Tracing

**Pergunta:**

> Como vocÃª testaria que distributed tracing (OpenTelemetry, Jaeger) estÃ¡ funcionando corretamente em um sistema com 5 microserviÃ§os?

**O que avaliar:**

- PropagaÃ§Ã£o de trace context
- Testes end-to-end
- ValidaÃ§Ã£o de spans
- Ferramentas (Jaeger UI, assertions)

---

#### P23: Correlation IDs

**Pergunta:**

> Por que correlation IDs sÃ£o importantes? Como vocÃª garantiria que eles sÃ£o propagados corretamente entre serviÃ§os? Como testaria isso?

**O que avaliar:**

- Entendimento de debugging distribuÃ­do
- ImplementaÃ§Ã£o (headers, MDC/ThreadLocal)
- Testes de propagaÃ§Ã£o
- IntegraÃ§Ã£o com logging

---

#### P24: MÃ©tricas de NegÃ³cio

**Pergunta:**

> AlÃ©m de mÃ©tricas tÃ©cnicas (latency, error rate), que mÃ©tricas de negÃ³cio vocÃª instrumentaria em um sistema de checkout? Como testaria a coleta dessas mÃ©tricas?

**O que avaliar:**

- VisÃ£o alÃ©m de mÃ©tricas tÃ©cnicas
- Exemplos concretos (conversion rate, cart abandonment)
- Testes de instrumentaÃ§Ã£o
- Ferramentas (Prometheus custom metrics)

---

#### P25: Health Checks

**Pergunta:**

> Projete um health check endpoint robusto. O que ele deve verificar? Como evitar que o health check cause mais problemas (ex: DDoS no banco)? Como testaria?

**O que avaliar:**

- VerificaÃ§Ãµes essenciais (DB, dependencies)
- Trade-offs (profundidade vs latÃªncia)
- Testes de falha
- PrÃ¡ticas (liveness vs readiness)

---

## 8. SeguranÃ§a

### ðŸŽ¯ NÃ­vel: SÃªnior

#### P26: Security Testing

**Pergunta:**

> Que tipos de testes de seguranÃ§a vocÃª incluiria em um pipeline CI/CD? Quais ferramentas open source usaria? Como balancearia seguranÃ§a e velocidade?

**O que avaliar:**

- SAST, DAST, dependency scanning
- Ferramentas (OWASP ZAP, Snyk, Trivy)
- IntegraÃ§Ã£o no pipeline
- Shift-left security

---

#### P27: SQL Injection Testing

**Pergunta:**

> Como vocÃª testaria que sua aplicaÃ§Ã£o estÃ¡ protegida contra SQL injection? Que tÃ©cnicas de prevenÃ§Ã£o validaria nos testes?

**O que avaliar:**

- Conhecimento de ataque e defesa
- Testes com payloads maliciosos
- Prepared statements, ORM
- Ferramentas (SQLMap para testes)

---

#### P28: Authentication & Authorization

**Pergunta:**

> Como vocÃª testaria regras de autorizaÃ§Ã£o complexas (RBAC, ABAC)? Que estratÃ©gias usaria para garantir cobertura completa?

**O que avaliar:**

- Matriz de permissÃµes
- Testes parametrizados
- Edge cases (privilege escalation)
- Ferramentas (Spring Security Test)

---

#### P29: Secrets Management

**Pergunta:**

> Como vocÃª garantiria que secrets nÃ£o vazam em logs ou mensagens de erro? Como testaria isso? Que prÃ¡ticas implementaria no time?

**O que avaliar:**

- EstratÃ©gias (masking, secret managers)
- Testes automatizados (grep logs)
- Code review practices
- Ferramentas (git-secrets, TruffleHog)

---

#### P30: Supply Chain Security

**Pergunta:**

> Como vocÃª garantiria a seguranÃ§a de dependÃªncias third-party? Que verificaÃ§Ãµes automatizadas implementaria? Como responderia a uma CVE crÃ­tica?

**O que avaliar:**

- SBOM, vulnerability scanning
- Ferramentas (Dependabot, Snyk, OWASP Dependency-Check)
- Processo de response
- Testes de regressÃ£o pÃ³s-update

---

## 9. Arquitetura e Design

### ðŸŽ¯ NÃ­vel: SÃªnior

#### P31: Testabilidade no Design

**Pergunta:**

> VocÃª estÃ¡ revisando o design de um novo serviÃ§o. Quais caracterÃ­sticas arquiteturais vocÃª buscaria para garantir alta testabilidade? DÃª exemplos de decisÃµes que facilitam ou dificultam testes.

**O que avaliar:**

- Dependency injection, interfaces
- SeparaÃ§Ã£o de concerns
- CÃ³digo testÃ¡vel vs acoplado
- RefatoraÃ§Ã£o para testabilidade

---

#### P32: Contract Testing

**Pergunta:**

> Quando contract testing (Pact) Ã© mais apropriado que integration testing? Como vocÃª implementaria contract tests entre 3 serviÃ§os (frontend, BFF, backend)?

**O que avaliar:**

- Entendimento de consumer-driven contracts
- Pact workflow (consumer â†’ provider)
- Trade-offs vs E2E
- Ferramentas (Pact, Spring Cloud Contract)

---

#### P33: Event-Driven Testing

**Pergunta:**

> Como vocÃª testaria um sistema event-driven (Kafka, RabbitMQ)? Que desafios vocÃª anteciparia? Quais estratÃ©gias usaria?

**O que avaliar:**

- Testes de produÃ§Ã£o/consumo
- IdempotÃªncia, ordering, duplicaÃ§Ã£o
- Testcontainers para Kafka
- Testes de falha (broker down)

---

#### P34: Database Migration Testing

**Pergunta:**

> Como vocÃª testaria migraÃ§Ãµes de banco de dados (Flyway, Liquibase)? Como garantiria rollback seguro? Que cenÃ¡rios de falha testaria?

**O que avaliar:**

- Testes de migraÃ§Ã£o up/down
- Dados existentes (backward compatibility)
- Performance de migraÃ§Ãµes
- CI/CD integration

---

#### P35: Hexagonal Architecture

**Pergunta:**

> Como a arquitetura hexagonal (ports & adapters) facilita testes? DÃª um exemplo de como vocÃª testaria a mesma lÃ³gica de negÃ³cio com diferentes adapters (HTTP vs message queue).

**O que avaliar:**

- Entendimento de hexagonal architecture
- SeparaÃ§Ã£o de concerns
- Testes de ports sem adapters
- SubstituiÃ§Ã£o de adapters

---

## 10. Processos e Cultura

### ðŸŽ¯ NÃ­vel: SÃªnior

#### P36: TDD Adoption

**Pergunta:**

> Seu time debate adotar TDD. VocÃª Ã© a favor ou contra? Em que contextos TDD agrega mais valor? Como vocÃª introduziria TDD gradualmente?

**O que avaliar:**

- Entendimento de TDD (Red-Green-Refactor)
- Pragmatismo (nÃ£o dogmatismo)
- EstratÃ©gia de adoÃ§Ã£o
- BenefÃ­cios e custos

---

#### P37: Code Review para Testes

**Pergunta:**

> Que aspectos vocÃª priorizaria ao revisar testes em um PR? DÃª 5 pontos de atenÃ§Ã£o crÃ­ticos.

**O que avaliar:**

- Nomenclatura, clareza
- Cobertura de edge cases
- Flakiness potencial
- Performance
- MutaÃ§Ã£o de conceitos crÃ­ticos

---

#### P38: Test Ownership

**Pergunta:**

> "QA deveria escrever todos os testes" vs "Desenvolvedores devem testar tudo". Qual modelo vocÃª prefere? Por quÃª? Como vocÃª dividiria responsabilidades?

**O que avaliar:**

- Shift-left mindset
- ColaboraÃ§Ã£o dev-QA
- PirÃ¢mide de testes
- Expertise apropriada

---

#### P39: Flaky Test Policy

**Pergunta:**

> Seu time discute a polÃ­tica: "Testes flaky devem ser desabilitados atÃ© serem corrigidos" vs "Testes flaky devem bloquear merges atÃ© serem corrigidos". Qual vocÃª escolheria?

**O que avaliar:**

- Trade-offs (velocidade vs qualidade)
- Contexto importa
- EstratÃ©gias intermediÃ¡rias
- Cultura de propriedade

---

#### P40: Testing Budget

**Pergunta:**

> VocÃª tem 2 semanas de capacidade para melhorar testes. Como priorizaria: aumentar cobertura, reduzir flakiness, adicionar mutation testing, ou implementar performance tests?

**O que avaliar:**

- AnÃ¡lise de risco
- ROI de diferentes abordagens
- Contexto do projeto
- MÃ©tricas para decisÃ£o

---

## 11. Trade-offs e DecisÃµes

### ðŸŽ¯ NÃ­vel: SÃªnior

#### P41: Test Speed vs Confidence

**Pergunta:**

> Seus testes de integraÃ§Ã£o levam 30 minutos. O time quer velocidade. VocÃª consideraria: paralelizaÃ§Ã£o, reduzir testes, ou substituir por mocks? Como decidiria?

**O que avaliar:**

- AnÃ¡lise de mÃºltiplas opÃ§Ãµes
- MÃ©tricas (tempo, confianÃ§a, custo)
- ImplementaÃ§Ã£o tÃ©cnica
- Trade-offs de cada abordagem

---

#### P42: Production Testing

**Pergunta:**

> "Testing in production" Ã© boa prÃ¡tica ou negligÃªncia? Quando Ã© apropriado? Que tÃ©cnicas vocÃª usaria (canary, feature flags, synthetic monitoring)?

**O que avaliar:**

- Entendimento de testes em produÃ§Ã£o
- TÃ©cnicas apropriadas
- Riscos e mitigaÃ§Ãµes
- Ferramentas (LaunchDarkly, canary deployments)

---

#### P43: Test Data em ProduÃ§Ã£o

**Pergunta:**

> VocÃª precisa testar em produÃ§Ã£o mas nÃ£o quer afetar dados reais. Que estratÃ©gias consideraria? Como garantiria isolamento?

**O que avaliar:**

- Shadow mode, synthetic users
- SeparaÃ§Ã£o lÃ³gica (flags, tagging)
- Limpeza de dados
- Compliance e privacidade

---

#### P44: Monorepo vs Multirepo Testing

**Pergunta:**

> Como a estratÃ©gia de testes muda entre monorepo e multirepo? Que desafios Ãºnicos cada abordagem apresenta?

**O que avaliar:**

- Testes cross-service
- CI/CD implications
- Dependency management
- Ferramentas (Nx, Bazel, etc.)

---

#### P45: Economic Trade-offs

**Pergunta:**

> Calcule o custo de: 1) Escrever mais testes (tempo dev), 2) Bugs em produÃ§Ã£o (downtime), 3) CI/CD infrastructure. Como vocÃª otimizaria esse balanÃ§o?

**O que avaliar:**

- VisÃ£o econÃ´mica (nÃ£o apenas tÃ©cnica)
- QuantificaÃ§Ã£o de riscos
- Dados para decisÃ£o
- MÃ©tricas DORA, SLA/SLO

---

#### P46: Legacy Code Testing

**Pergunta:**

> VocÃª herdou 100k linhas de cÃ³digo sem testes. Qual sua estratÃ©gia? Por onde comeÃ§ar? Como medir progresso?

**O que avaliar:**

- Characterization tests
- RefatoraÃ§Ã£o segura
- PriorizaÃ§Ã£o por risco
- Working Effectively with Legacy Code (Feathers)

---

#### P47: Test Frameworks Selection

**Pergunta:**

> VocÃª estÃ¡ iniciando um novo projeto Python. Como decidiria entre pytest, unittest, nose? Que critÃ©rios usaria?

**O que avaliar:**

- Conhecimento de ferramentas
- CritÃ©rios objetivos (features, comunidade)
- Contexto do time
- Open source ecosystem

---

#### P48: AI-Generated Tests

**Pergunta:**

> Ferramentas de IA (Copilot, ChatGPT) podem gerar testes. Quando vocÃª confiaria neles? Quando revisaria criticamente? Que riscos anteciparia?

**O que avaliar:**

- Pragmatismo com novas tecnologias
- Entendimento de limitaÃ§Ãµes
- RevisÃ£o crÃ­tica
- Complemento, nÃ£o substituiÃ§Ã£o

---

#### P49: Observability vs Testing

**Pergunta:**

> "Observabilidade suficiente elimina necessidade de testes extensivos". Concorda ou discorda? Como eles se complementam?

**O que avaliar:**

- Entendimento de diferenÃ§as
- Complementaridade
- Shift-left (testes) vs shift-right (observability)
- Balanced approach

---

#### P50: Open Source Contribution

**Pergunta:**

> VocÃª encontrou um bug em uma biblioteca open source (ex: Mockito). Como decidiria entre: workaround, fork, ou contribuir fix upstream? Como testaria o fix?

**O que avaliar:**

- VisÃ£o de comunidade
- Pragmatismo vs idealismo
- ContribuiÃ§Ã£o para ecosystem
- Processo de OSS contribution

---

## ðŸ“Š Matriz de ClassificaÃ§Ã£o das Perguntas

| Categoria           | NÃ­vel Pleno        | NÃ­vel SÃªnior                                     | Total  |
| ------------------- | ------------------ | ------------------------------------------------ | ------ |
| **Fundamentos**     | P1, P2, P3, P4, P5 | -                                                | 5      |
| **Qualidade**       | P6, P7, P8         | P9, P10                                          | 5      |
| **ResiliÃªncia**     | -                  | P11, P12, P13, P14, P15                          | 5      |
| **Performance**     | -                  | P16, P17, P18, P19, P20                          | 5      |
| **Observabilidade** | -                  | P21, P22, P23, P24, P25                          | 5      |
| **SeguranÃ§a**       | -                  | P26, P27, P28, P29, P30                          | 5      |
| **Arquitetura**     | -                  | P31, P32, P33, P34, P35                          | 5      |
| **Processos**       | -                  | P36, P37, P38, P39, P40                          | 5      |
| **Trade-offs**      | -                  | P41, P42, P43, P44, P45, P46, P47, P48, P49, P50 | 10     |
| **TOTAL**           | **13**             | **37**                                           | **50** |

---

## ðŸŽ¯ Guia de SeleÃ§Ã£o de Perguntas

### Para NÃ­vel Pleno

**Foco:** Fundamentos, ferramentas, prÃ¡ticas bÃ¡sicas

**SugestÃ£o de Combo (45 min):**

1. P2 (Test Doubles) - 10 min
2. P3 (Flaky Tests) - 10 min
3. P6 (Code Coverage) - 10 min
4. P36 (TDD) - 10 min
5. Mini-caso prÃ¡tico - 5 min

### Para NÃ­vel SÃªnior

**Foco:** Arquitetura, trade-offs, visÃ£o sistÃªmica

**SugestÃ£o de Combo (60 min):**

1. P11 (Circuit Breaker) - 12 min
2. P21 (Logs/MÃ©tricas/Traces) - 12 min
3. P31 (Testabilidade no Design) - 12 min
4. P41 (Speed vs Confidence) - 12 min
5. P45 (Economic Trade-offs) - 12 min

### Para Arquiteto/Principal

**Foco:** VisÃ£o estratÃ©gica, influÃªncia, decisÃµes de longo prazo

**SugestÃ£o de Combo (90 min):**

1. P9 (Quality Gates) - 15 min
2. P14 (Chaos Engineering) - 15 min
3. P32 (Contract Testing) - 15 min
4. P40 (Testing Budget) - 15 min
5. P45 (Economic Trade-offs) - 15 min
6. Mini-caso de arquitetura - 15 min

---

## ðŸ” Exemplo de Resposta Completa

### Pergunta: P11 (Circuit Breaker)

**Resposta Exemplo (NÃ­vel SÃªnior):**

**Contexto:**
"No Ãºltimo projeto, implementei Circuit Breaker para chamadas ao serviÃ§o de pagamento PagSeguro, que tinha SLA de 99.5% mas sofria picos de latÃªncia."

**Problema:**
"Quando o PagSeguro ficava lento (>5s), nossa API tambÃ©m travava, causando timeout em cascata. PrecisÃ¡vamos isolar essa falha."

**SoluÃ§Ã£o:**
"Usei Resilience4j com configuraÃ§Ã£o:

- Threshold: 50% de falhas em 10 chamadas
- Timeout: 2s por chamada
- Wait duration: 30s no estado Open
- Half-open: testar com 3 chamadas

Para testes:

1. **Unit test (estados):** Mockei o serviÃ§o para retornar falhas e validei transiÃ§Ã£o Closedâ†’Openâ†’Half-Openâ†’Closed
2. **Integration test:** Usei WireMock com delays para simular latÃªncia e validar timeout
3. **Chaos test:** Toxiproxy para simular falha total do PagSeguro

MÃ©tricas monitoradas:

- `resilience4j.circuitbreaker.state` (gauge: 0=closed, 1=open, 2=half-open)
- `resilience4j.circuitbreaker.calls` (counter por resultado: success/failure/fallback)
- Alertas no Grafana quando estado = Open por >5min"

**Resultado:**
"Reduzimos timeout em cascata de 45% para <5%. P99 latency melhorou de 8s para 2.5s mesmo durante incidentes do PagSeguro."

**Aprendizado:**
"Inicialmente configurei threshold muito agressivo (20%), causando circuit aberto desnecessÃ¡rio. Ajustei para 50% apÃ³s anÃ¡lise de mÃ©tricas. TambÃ©m aprendi a importÃ¢ncia de fallback: retornar erro 503 com retry-after header vs 500 genÃ©rico."

---

## âœ… Checklist para AutoavaliaÃ§Ã£o

Use este checklist para medir sua preparaÃ§Ã£o:

### Fundamentos (Pleno)

- [ ] Consigo explicar diferenÃ§a entre unit/integration/E2E com exemplos
- [ ] Sei usar Mockito (mock, spy, stub, verify, ArgumentCaptor)
- [ ] Identifico e corrijo flaky tests
- [ ] Uso Test Data Builders para cenÃ¡rios complexos
- [ ] Sigo estrutura AAA/Given-When-Then consistentemente

### Qualidade (Pleno â†’ SÃªnior)

- [ ] Entendo limitaÃ§Ãµes de code coverage
- [ ] Implementei mutation testing (PITest/Stryker)
- [ ] Configurei diff coverage no CI/CD
- [ ] Projetei quality gates em mÃºltiplas camadas
- [ ] Sei balancear pirÃ¢mide de testes

### ResiliÃªncia (SÃªnior)

- [ ] Implementei Circuit Breaker em produÃ§Ã£o
- [ ] Testei polÃ­ticas de Retry com exponential backoff
- [ ] Garanti idempotÃªncia em operaÃ§Ãµes crÃ­ticas
- [ ] Realizei experimentos de Chaos Engineering
- [ ] Defini e testei timeout strategies

### Performance (SÃªnior)

- [ ] Realizei load/stress/spike tests (JMeter/k6)
- [ ] Defini performance budgets mensurÃ¡veis
- [ ] Identifiquei e resolvi N+1 queries
- [ ] Testei cache (TTL, invalidation, hit rate)
- [ ] Otimizei queries com EXPLAIN ANALYZE

### Observabilidade (SÃªnior)

- [ ] Implementei distributed tracing (Jaeger/OpenTelemetry)
- [ ] Testei propagaÃ§Ã£o de correlation IDs
- [ ] Instrumentei mÃ©tricas de negÃ³cio
- [ ] Projetei health checks robustos
- [ ] Uso logs/mÃ©tricas/traces apropriadamente

### SeguranÃ§a (SÃªnior)

- [ ] Integrei security testing no CI/CD (SAST/DAST)
- [ ] Testei proteÃ§Ãµes contra SQL injection
- [ ] Validei regras de autorizaÃ§Ã£o complexas
- [ ] Implementei secret masking em logs
- [ ] Monitoro vulnerabilidades (SBOM, CVEs)

### Arquitetura (SÃªnior)

- [ ] Projeto cÃ³digo pensando em testabilidade
- [ ] Implementei contract testing (Pact)
- [ ] Testei sistemas event-driven (Kafka)
- [ ] Validei database migrations (up/down)
- [ ] Aplico hexagonal architecture

### Processos (SÃªnior)

- [ ] Pratico TDD quando apropriado
- [ ] Reviso testes em PRs com critÃ©rios claros
- [ ] Defendo cultura de qualidade no time
- [ ] Tenho polÃ­tica para flaky tests
- [ ] Sei priorizar investimento em testes

### Trade-offs (SÃªnior)

- [ ] BalanÃ§o speed vs confidence baseado em contexto
- [ ] Uso testing in production apropriadamente
- [ ] Considero trade-offs econÃ´micos
- [ ] Tenho estratÃ©gia para legacy code
- [ ] Avalio ferramentas com critÃ©rios objetivos

**PontuaÃ§Ã£o:**

- 40-45 âœ…: Pronto para sÃªnior
- 30-39 ðŸ”¶: Caminho certo, focar em gaps
- 20-29 ðŸ“š: Estudo aprofundado necessÃ¡rio
- <20 ðŸŽ¯: Consolidar fundamentos primeiro

---

## ðŸ“š Recursos Complementares

### Livros

- **Growing Object-Oriented Software, Guided by Tests** (Freeman, Pryce)
- **Working Effectively with Legacy Code** (Feathers)
- **Release It!** (Nygard) - Patterns de resiliÃªncia
- **Building Microservices** (Newman) - Testing distribuÃ­do

### Ferramentas Open Source

- **Testes:** JUnit 5, pytest, Jest, TestNG
- **Mocking:** Mockito, unittest.mock, Sinon
- **Mutation:** PITest, Stryker, mutmut
- **Coverage:** JaCoCo, Coverage.py, Istanbul
- **Performance:** JMeter, Gatling, k6, Locust
- **Chaos:** Chaos Toolkit, Pumba, Toxiproxy
- **Contract:** Pact, Spring Cloud Contract
- **Containers:** Testcontainers
- **Observability:** Prometheus, Grafana, Jaeger

### ReferÃªncias

- [Martin Fowler - Testing](https://martinfowler.com/testing/)
- [Google Testing Blog](https://testing.googleblog.com/)
- [Microsoft - Testing Pyramid](https://docs.microsoft.com/en-us/azure/devops/learn/devops-at-microsoft/shift-left-make-testing-fast-reliable)

---

**PrÃ³ximo passo:** Praticar com [mini-casos](mini-casos.md) e avaliar usando [rubrica](rubrica-avaliacao.md).
