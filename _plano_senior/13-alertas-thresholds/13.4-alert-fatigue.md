# üîï M√≥dulo 13.4: Alert Fatigue e On-Call

> **Objetivo:** Reduzir alert fatigue, automatizar runbooks e otimizar on-call rotation.

---

## üìë √çndice

1. [Alert Fatigue](#1-alert-fatigue)
2. [Alert Tuning](#2-alert-tuning)
3. [Runbook Automation](#3-runbook-automation)
4. [On-Call Rotation](#4-on-call-rotation)
5. [Post-Mortem](#5-post-mortem)

---

## 1. Alert Fatigue

### 1.1 O Problema

```java
/**
 * ALERT FATIGUE: Dessensibiliza√ß√£o por excesso de alertas
 *
 * Sintomas:
 * - Ignorar alertas cr√≠ticos (cry wolf effect)
 * - Burnout da equipe on-call
 * - Atrasos na resposta (MTTR alto)
 * - Snooze/silence indiscriminado
 *
 * Causas:
 * - Falsos positivos (thresholds mal calibrados)
 * - Alertas n√£o acion√°veis (sem a√ß√£o clara)
 * - Ru√≠do (alertas secund√°rios de mesma causa)
 * - Duplica√ß√£o (m√∫ltiplos alertas para mesmo problema)
 */

/**
 * M√âTRICAS DE ALERT FATIGUE
 */
@Service
public class AlertFatigueMetrics {

    @Autowired
    private MeterRegistry registry;

    /**
     * Taxa de falsos positivos
     *
     * F√≥rmula: (alertas resolvidos sem a√ß√£o / total alertas) * 100
     *
     * Target: < 5%
     */
    public double calculateFalsePositiveRate(List<Alert> alerts) {
        var falsePositives = alerts.stream()
                                   .filter(a -> a.getResolution() == Resolution.NO_ACTION)
                                   .count();

        return (double) falsePositives / alerts.size() * 100;
    }

    /**
     * Alert noise ratio
     *
     * F√≥rmula: (alertas info/warning / alertas critical) * 100
     *
     * Target: < 200% (no m√°ximo 2 warning/info por 1 critical)
     */
    public double calculateNoiseRatio(List<Alert> alerts) {
        var criticalCount = alerts.stream()
                                  .filter(a -> a.getSeverity() == Severity.CRITICAL)
                                  .count();

        var nonCriticalCount = alerts.stream()
                                     .filter(a -> a.getSeverity() != Severity.CRITICAL)
                                     .count();

        return (double) nonCriticalCount / criticalCount * 100;
    }

    /**
     * MTTA (Mean Time to Acknowledge)
     *
     * Target: < 5 minutos para critical
     */
    public Duration calculateMTTA(List<Alert> alerts) {
        var totalAckTime = alerts.stream()
                                 .map(a -> Duration.between(a.getFiredAt(), a.getAcknowledgedAt()))
                                 .reduce(Duration.ZERO, Duration::plus);

        return totalAckTime.dividedBy(alerts.size());
    }

    /**
     * MTTR (Mean Time to Resolve)
     *
     * Target: < 30 minutos para critical
     */
    public Duration calculateMTTR(List<Alert> alerts) {
        var totalResolveTime = alerts.stream()
                                     .map(a -> Duration.between(a.getFiredAt(), a.getResolvedAt()))
                                     .reduce(Duration.ZERO, Duration::plus);

        return totalResolveTime.dividedBy(alerts.size());
    }
}
```

---

### 1.2 Alert Hygiene

```yaml
# alert-rules-hygiene.yml

groups:
  - name: alert_quality
    interval: 1m
    rules:
      # M√©trica: Total de alertas firing
      - record: alerts:firing:count
        expr: |
          count(ALERTS{alertstate="firing"})

      # M√©trica: Alertas critical firing
      - record: alerts:critical:count
        expr: |
          count(ALERTS{alertstate="firing", severity="critical"})

      # Alert: Muitos alertas firing (> 50)
      - alert: AlertStorm
        expr: |
          alerts:firing:count > 50
        for: 5m
        labels:
          severity: critical
          meta: true # Meta-alert (alerta sobre alertas)
        annotations:
          summary: "Alert storm detected (>50 alerts firing)"
          action: |
            1. Check for root cause (instance down, deploy issue)
            2. Silence non-critical alerts temporarily
            3. Focus on critical alerts only

      # Alert: MTTA muito alto (> 10min)
      - alert: SlowAlertAcknowledgement
        expr: |
          avg_over_time(alertmanager_notification_latency_seconds[1h]) > 600
        for: 1h
        labels:
          severity: warning
          meta: true
        annotations:
          summary: "Slow alert acknowledgement (MTTA > 10min)"
          action: "Review on-call rotation, check PagerDuty integration"
```

---

## 2. Alert Tuning

### 2.1 Calibra√ß√£o de Thresholds

```java
/**
 * PROCESSO DE CALIBRA√á√ÉO
 *
 * 1. Coletar dados hist√≥ricos (30 dias)
 * 2. Calcular baseline e stddev
 * 3. Definir threshold: baseline + N * stddev
 * 4. Validar com dados hist√≥ricos (false positive rate)
 * 5. Ajustar N at√© FP < 5%
 */

@Service
public class ThresholdCalibrationService {

    @Autowired
    private PrometheusClient prometheus;

    /**
     * Calibrar threshold automaticamente
     */
    public ThresholdRecommendation calibrateThreshold(String metric, int daysBack) {
        // 1. Coletar dados hist√≥ricos
        var historicalData = prometheus.queryRange(
            metric,
            Instant.now().minus(daysBack, ChronoUnit.DAYS),
            Instant.now()
        );

        // 2. Calcular estat√≠sticas
        var baseline = historicalData.stream()
                                     .mapToDouble(Point::getValue)
                                     .average()
                                     .orElse(0);

        var stddev = calculateStdDev(historicalData);

        // 3. Testar m√∫ltiplos N (1, 2, 3, 4, 5)
        var bestN = 0.0;
        var bestFalsePositiveRate = 100.0;

        for (double n = 1.0; n <= 5.0; n += 0.5) {
            var threshold = baseline + (n * stddev);
            var falsePositiveRate = simulateFalsePositives(historicalData, threshold);

            if (falsePositiveRate < bestFalsePositiveRate && falsePositiveRate < 5.0) {
                bestN = n;
                bestFalsePositiveRate = falsePositiveRate;
            }
        }

        var recommendedThreshold = baseline + (bestN * stddev);

        return new ThresholdRecommendation(
            metric,
            baseline,
            stddev,
            bestN,
            recommendedThreshold,
            bestFalsePositiveRate
        );
    }

    private double simulateFalsePositives(List<Point> data, double threshold) {
        var violations = data.stream()
                             .filter(p -> p.getValue() > threshold)
                             .count();

        // Assumir que 90% das viola√ß√µes s√£o reais problemas
        var falsePositives = violations * 0.1;

        return (falsePositives / data.size()) * 100;
    }
}

record ThresholdRecommendation(
    String metric,
    double baseline,
    double stddev,
    double multiplier,
    double recommendedThreshold,
    double expectedFalsePositiveRate
) {}
```

---

### 2.2 Alert Tagging

```yaml
# alert-rules-tagged.yml

groups:
  - name: tagged_alerts
    interval: 1m
    rules:
      - alert: HighLatency
        expr: p99_latency > 500
        labels:
          severity: warning
          component: api
          team: backend
          runbook: "https://wiki/runbooks/high-latency"
          slo: "latency_p99"
          customer_impact: "medium" # low / medium / high
          automated_action: "false" # true = pode ser auto-remediado
        annotations:
          summary: "High latency detected"
          description: "p99 = {{ $value }}ms"
          triage_guide: |
            1. Check slow queries (pg_stat_statements)
            2. Check N+1 queries (APM traces)
            3. Check cache hit rate
          escalation_policy: "backend-team -> platform-team"
```

---

## 3. Runbook Automation

### 3.1 Runbook Template

````markdown
# Runbook: HighErrorRate

## Metadata

- **Severity:** Critical
- **Owner:** Backend Team
- **MTTA Target:** < 5 min
- **MTTR Target:** < 30 min

## Symptoms

- Error rate > 5%
- Users reporting 500 errors
- Dashboard shows spike in `http_requests_total{status="500"}`

## Triage (5 minutes)

### 1. Check recent deploys

```bash
kubectl rollout history deployment/app
```
````

Se deploy recente (< 2h), suspeitar de deploy ruim.

### 2. Check error logs

```bash
kubectl logs -l app=myapp --tail=100 | grep ERROR
```

Identificar exception mais frequente.

### 3. Check dependencies

```bash
# Database
kubectl get pods -l app=postgres
# Redis
kubectl get pods -l app=redis
# External APIs
curl -I https://api.external.com/health
```

## Mitigation (10 minutes)

### Caso 1: Deploy recente

```bash
# Rollback imediato
kubectl rollout undo deployment/app
```

### Caso 2: Database down

```bash
# Ativar circuit breaker (se n√£o autom√°tico)
curl -X POST http://app:8080/actuator/circuitbreakers/database/open
```

### Caso 3: Overload

```bash
# Escalar pods
kubectl scale deployment app --replicas=10
```

## Resolution (30 minutes)

1. Identificar root cause (logs, APM, database)
2. Deploy com fix
3. Validar error rate < 1%
4. Comunicar incidente (Slack #incidents)

## Post-Mortem

- [ ] Documento de incident criado
- [ ] 5 Whys para root cause
- [ ] Action items criados (JIRA)
- [ ] Post-mortem meeting agendado

## Automation Opportunities

- [ ] Auto-rollback em error rate > 10%
- [ ] Auto-scaling baseado em error rate
- [ ] Circuit breaker autom√°tico

````

---

### 3.2 Auto-Remediation

```java
/**
 * Auto-remediation: A√ß√µes automatizadas em alertas
 */
@Service
public class AutoRemediationService {

    @Autowired
    private KubernetesClient k8s;

    @Autowired
    private Resilience4jClient resilience4j;

    /**
     * Alert Webhook: AlertManager chama este endpoint
     */
    @PostMapping("/webhooks/alertmanager")
    public void handleAlert(@RequestBody AlertWebhook webhook) {
        for (var alert : webhook.getAlerts()) {
            if (alert.getStatus().equals("firing")) {
                autoRemediate(alert);
            }
        }
    }

    /**
     * Auto-remediar baseado em alert
     */
    private void autoRemediate(Alert alert) {
        var action = alert.getLabels().get("automated_action");

        if (!"true".equals(action)) {
            return;  // N√£o automatizado
        }

        switch (alert.getName()) {
            case "HighCPU" -> {
                // Auto-scale pods
                var deployment = alert.getLabels().get("deployment");
                k8s.scale(deployment, 10);
                log.info("Auto-scaled {} to 10 replicas", deployment);
            }

            case "DatabaseDown" -> {
                // Abrir circuit breaker
                resilience4j.openCircuitBreaker("database");
                log.info("Opened circuit breaker for database");
            }

            case "HighErrorRate" -> {
                // Rollback se deploy recente
                var deployment = alert.getLabels().get("deployment");
                var lastDeployTime = k8s.getLastDeployTime(deployment);

                if (Duration.between(lastDeployTime, Instant.now()).toMinutes() < 120) {
                    k8s.rollback(deployment);
                    log.warn("Auto-rolled back {} due to high error rate", deployment);
                }
            }

            case "MemoryLeak" -> {
                // Restart pods gradualmente
                var deployment = alert.getLabels().get("deployment");
                k8s.rollingRestart(deployment);
                log.warn("Rolling restart of {} due to memory leak", deployment);
            }
        }
    }
}
````

---

## 4. On-Call Rotation

### 4.1 On-Call Schedule

```java
/**
 * On-call rotation management
 */
@Service
public class OnCallService {

    private static final Map<DayOfWeek, String> ROTATION = Map.of(
        DayOfWeek.MONDAY, "alice@company.com",
        DayOfWeek.TUESDAY, "bob@company.com",
        DayOfWeek.WEDNESDAY, "charlie@company.com",
        DayOfWeek.THURSDAY, "diana@company.com",
        DayOfWeek.FRIDAY, "alice@company.com",
        DayOfWeek.SATURDAY, "bob@company.com",
        DayOfWeek.SUNDAY, "charlie@company.com"
    );

    /**
     * Quem est√° on-call agora?
     */
    public String getCurrentOnCall() {
        return ROTATION.get(LocalDate.now().getDayOfWeek());
    }

    /**
     * Escalation policy: Prim√°rio -> Secund√°rio -> Manager
     */
    public List<String> getEscalationChain(Alert alert) {
        var primary = getCurrentOnCall();
        var secondary = getSecondaryOnCall();
        var manager = "manager@company.com";

        return List.of(primary, secondary, manager);
    }

    /**
     * Handoff: Resumo para pr√≥ximo on-call
     */
    public OnCallHandoff generateHandoff() {
        var activeIncidents = getActiveIncidents();
        var recentAlerts = getRecentAlerts(Duration.ofHours(24));
        var actionItems = getPendingActionItems();

        return new OnCallHandoff(
            LocalDateTime.now(),
            getCurrentOnCall(),
            activeIncidents,
            recentAlerts,
            actionItems
        );
    }
}

record OnCallHandoff(
    LocalDateTime timestamp,
    String outgoingEngineer,
    List<Incident> activeIncidents,
    List<Alert> recentAlerts,
    List<ActionItem> pendingItems
) {}
```

---

### 4.2 PagerDuty Integration

```yaml
# alertmanager-pagerduty-rotation.yml

receivers:
  - name: "pagerduty-oncall"
    pagerduty_configs:
      - service_key: "PRIMARY_ONCALL_KEY"
        description: "{{ .GroupLabels.alertname }}: {{ .CommonAnnotations.summary }}"
        severity: "{{ .CommonLabels.severity }}"

        # Escalation ap√≥s 15min sem ACK
        details:
          escalation_policy: "backend-team-escalation"
          urgency: "high"

        # Incluir runbook link
        client_url: "{{ .CommonLabels.runbook }}"
```

```java
/**
 * PagerDuty API: Escalation
 */
@Service
public class PagerDutyEscalationService {

    @Autowired
    private RestTemplate restTemplate;

    @Value("${pagerduty.api-token}")
    private String apiToken;

    /**
     * Escalar incident se n√£o resolvido em 15min
     */
    @Scheduled(fixedRate = 60000)  // A cada 1min
    public void checkUnacknowledgedIncidents() {
        var incidents = getUnacknowledgedIncidents();

        for (var incident : incidents) {
            var duration = Duration.between(incident.getCreatedAt(), Instant.now());

            if (duration.toMinutes() > 15) {
                escalateIncident(incident);
            }
        }
    }

    private void escalateIncident(Incident incident) {
        var escalationUrl = "https://api.pagerduty.com/incidents/" + incident.getId() + "/escalate";

        var headers = new HttpHeaders();
        headers.set("Authorization", "Token token=" + apiToken);
        headers.set("Content-Type", "application/json");

        var body = Map.of(
            "escalation_level", 2  // Pr√≥ximo n√≠vel
        );

        restTemplate.postForEntity(escalationUrl, new HttpEntity<>(body, headers), String.class);

        log.warn("Escalated incident {} to level 2", incident.getId());
    }
}
```

---

## 5. Post-Mortem

### 5.1 Post-Mortem Template

```markdown
# Post-Mortem: [Incident Title]

## Metadata

- **Date:** 2025-11-15
- **Duration:** 45 minutes (10:30 - 11:15 UTC)
- **Severity:** SEV-1 (Critical)
- **Owner:** Alice (Backend Team)
- **Reviewers:** Bob, Charlie, Diana

## Summary

High error rate (15%) caused by database connection pool exhaustion after deploy.

## Timeline (UTC)

| Time  | Event                                          |
| ----- | ---------------------------------------------- |
| 10:30 | Deploy v1.2.3 to production                    |
| 10:32 | Error rate spikes to 5%                        |
| 10:35 | PagerDuty alert fires (HighErrorRate)          |
| 10:37 | On-call acknowledges, starts triage            |
| 10:40 | Identified: Database connection pool exhausted |
| 10:42 | Mitigation: Scaled app pods (5 ‚Üí 10)           |
| 10:45 | Error rate drops to 2%                         |
| 10:50 | Root cause: New feature using N+1 queries      |
| 11:00 | Deploy v1.2.4 with fix (batch queries)         |
| 11:15 | Error rate back to normal (< 0.1%)             |
| 11:20 | Incident resolved                              |

## Impact

- **Users Affected:** ~5,000 users (10% of total)
- **Error Budget:** Consumed 12% of monthly budget
- **Revenue Impact:** $2,000 estimated loss
- **SLA Breach:** No (99.95% SLO maintained)

## Root Cause (5 Whys)

1. **Why did error rate spike?**  
   Database connection pool exhausted.

2. **Why was pool exhausted?**  
   New feature made N+1 queries (1 query per item).

3. **Why N+1 queries?**  
   Developer used `List<Item>` without `@BatchSize`.

4. **Why wasn't this caught in testing?**  
   Load tests only had 10 items, not realistic 100+ items.

5. **Why load tests inadequate?**  
   Load test data not representative of production.

**Root Cause:** N+1 query pattern + inadequate load testing.

## Resolution

- Deployed fix with `@BatchSize(size = 100)` annotation
- Error rate returned to normal in 15 minutes

## Action Items

| Item                                      | Owner   | Deadline   | Status  |
| ----------------------------------------- | ------- | ---------- | ------- |
| Add N+1 query detection to CI/CD          | Alice   | 2025-11-20 | ‚è≥ TODO |
| Improve load test data (100+ items)       | Bob     | 2025-11-22 | ‚è≥ TODO |
| Add connection pool saturation alert      | Charlie | 2025-11-18 | ‚è≥ TODO |
| Document N+1 anti-pattern in wiki         | Diana   | 2025-11-19 | ‚è≥ TODO |
| Code review checklist: Check `@BatchSize` | Alice   | 2025-11-17 | ‚úÖ DONE |

## Lessons Learned

### What Went Well

- ‚úÖ Alert fired within 2 minutes (MTTA target met)
- ‚úÖ On-call responded quickly (5min to acknowledge)
- ‚úÖ Mitigation (scaling) effective immediately
- ‚úÖ Fix deployed in 25 minutes (MTTR target met)

### What Went Wrong

- ‚ùå N+1 query not caught in code review
- ‚ùå Load tests not representative of production
- ‚ùå No alert for connection pool saturation

### What Got Lucky

- üçÄ Incident during business hours (not 3 AM)
- üçÄ On-call engineer familiar with codebase
- üçÄ Fix simple (1 annotation change)

## Prevention

- [ ] **Detection:** N+1 query linter in CI/CD (Hibernate Statistics)
- [ ] **Testing:** Load tests with production-like data
- [ ] **Monitoring:** Connection pool saturation alert
- [ ] **Documentation:** N+1 anti-pattern in wiki
```

---

## üìä Checklist de Qualidade

- [ ] False positive rate < 5%
- [ ] Alert noise ratio < 200%
- [ ] MTTA < 5min para critical alerts
- [ ] MTTR < 30min para critical alerts
- [ ] Runbooks documentados para todos alertas critical
- [ ] Auto-remediation para alertas comuns (scaling, circuit breaker)
- [ ] On-call rotation definida (PagerDuty/Opsgenie)
- [ ] Escalation policy (prim√°rio ‚Üí secund√°rio ‚Üí manager)
- [ ] Handoff process entre turnos on-call
- [ ] Post-mortem para todos SEV-1 incidents

---

## üéØ Exerc√≠cios Pr√°ticos

1. **B√°sico**: Calcular MTTA e MTTR dos √∫ltimos 30 dias
2. **Intermedi√°rio**: Calibrar threshold automaticamente (FP < 5%)
3. **Avan√ßado**: Implementar auto-remediation (rollback em error rate > 10%)

---

## üìö Refer√™ncias

- [Google SRE - Dealing with Alert Fatigue](https://sre.google/sre-book/monitoring-distributed-systems/)
- [PagerDuty Incident Response](https://response.pagerduty.com/)
- [Atlassian Post-Mortem Template](https://www.atlassian.com/incident-management/postmortem)

---

**Anterior:** [13.3 - Thresholds Din√¢micos](13.3-thresholds-dinamicos.md)  
**Pr√≥ximo:** [13.5 - Testing Alerts](13.5-testing-alerts.md)
