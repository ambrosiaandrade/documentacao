# ðŸ“¡ MÃ³dulo 13.2: Prometheus Alerting

> **Objetivo:** Dominar AlertManager, PromQL para alertas, routing e inhibition rules.

---

## ðŸ“‘ Ãndice

1. [AlertManager Setup](#1-alertmanager-setup)
2. [PromQL para Alertas](#2-promql-para-alertas)
3. [Routing e Grouping](#3-routing-e-grouping)
4. [Inhibition Rules](#4-inhibition-rules)
5. [Integrations](#5-integrations)

---

## 1. AlertManager Setup

### 1.1 Arquitetura

```yaml
# docker-compose.yml

version: "3.8"

services:
  prometheus:
    image: prom/prometheus:v2.48.0
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
      - ./alert-rules.yml:/etc/prometheus/alert-rules.yml
    command:
      - "--config.file=/etc/prometheus/prometheus.yml"
      - "--web.enable-lifecycle"

  alertmanager:
    image: prom/alertmanager:v0.26.0
    ports:
      - "9093:9093"
    volumes:
      - ./alertmanager.yml:/etc/alertmanager/alertmanager.yml
    command:
      - "--config.file=/etc/alertmanager/alertmanager.yml"
      - "--storage.path=/alertmanager"
```

```yaml
# prometheus.yml

global:
  scrape_interval: 15s
  evaluation_interval: 15s

# AlertManager config
alerting:
  alertmanagers:
    - static_configs:
        - targets:
            - "alertmanager:9093"

# Load alert rules
rule_files:
  - "alert-rules.yml"

scrape_configs:
  - job_name: "spring-boot-app"
    metrics_path: "/actuator/prometheus"
    static_configs:
      - targets:
          - "app:8080"
```

---

### 1.2 AlertManager Config

```yaml
# alertmanager.yml

global:
  # Tempo de resoluÃ§Ã£o (alerta nÃ£o firing)
  resolve_timeout: 5m

  # SMTP para emails
  smtp_smarthost: "smtp.gmail.com:587"
  smtp_from: "alerts@company.com"
  smtp_auth_username: "alerts@company.com"
  smtp_auth_password: "password"
  smtp_require_tls: true

# Templates para mensagens
templates:
  - "/etc/alertmanager/templates/*.tmpl"

# Routing tree
route:
  # Default receiver
  receiver: "team-default"

  # Agrupar alertas por...
  group_by: ["alertname", "cluster", "service"]

  # Esperar 30s antes de enviar (batch)
  group_wait: 30s

  # Re-enviar alerta se ainda ativo (5min)
  group_interval: 5m

  # Re-notificar a cada 4h se nÃ£o resolvido
  repeat_interval: 4h

  # Sub-rotas por severidade
  routes:
    # Critical: PagerDuty + Slack
    - match:
        severity: critical
      receiver: "pagerduty-critical"
      continue: true # Continua para outras rotas

    - match:
        severity: critical
      receiver: "slack-critical"

    # Warning: Slack apenas
    - match:
        severity: warning
      receiver: "slack-warnings"

    # Info: Email apenas
    - match:
        severity: info
      receiver: "email-info"

# Receivers (destinos)
receivers:
  - name: "team-default"
    email_configs:
      - to: "team@company.com"

  - name: "pagerduty-critical"
    pagerduty_configs:
      - service_key: "YOUR_PAGERDUTY_SERVICE_KEY"
        description: "{{ .GroupLabels.alertname }}"

  - name: "slack-critical"
    slack_configs:
      - api_url: "https://hooks.slack.com/services/YOUR/WEBHOOK/URL"
        channel: "#alerts-critical"
        title: "ðŸš¨ CRITICAL ALERT"
        text: |
          *Alert:* {{ .GroupLabels.alertname }}
          *Severity:* {{ .CommonLabels.severity }}
          *Summary:* {{ .CommonAnnotations.summary }}
          *Description:* {{ .CommonAnnotations.description }}
        send_resolved: true

  - name: "slack-warnings"
    slack_configs:
      - api_url: "https://hooks.slack.com/services/YOUR/WEBHOOK/URL"
        channel: "#alerts-warnings"
        title: "âš ï¸ Warning"
        send_resolved: true

  - name: "email-info"
    email_configs:
      - to: "team@company.com"
        headers:
          Subject: "Info Alert: {{ .GroupLabels.alertname }}"

# Inhibition rules (silenciar alertas dependentes)
inhibit_rules:
  # Se InstanceDown, silencia outros alertas daquela instÃ¢ncia
  - source_match:
      alertname: "InstanceDown"
    target_match_re:
      alertname: ".*"
    equal: ["instance"]

  # Se DatabaseDown, silencia HighErrorRate
  - source_match:
      alertname: "DatabaseDown"
    target_match:
      alertname: "HighErrorRate"
    equal: ["service"]
```

---

## 2. PromQL para Alertas

### 2.1 Alert Rules

```yaml
# alert-rules.yml

groups:
  - name: application_alerts
    interval: 30s
    rules:
      # ==================================================
      # LATENCY
      # ==================================================

      - alert: HighLatencyP99
        expr: |
          histogram_quantile(0.99, 
            rate(http_server_requests_seconds_bucket{job="spring-boot-app"}[5m])
          ) > 0.5
        for: 5m
        labels:
          severity: warning
          component: api
        annotations:
          summary: "High latency p99 on {{ $labels.instance }}"
          description: "p99 latency is {{ $value }}s (threshold: 0.5s)"
          runbook: "https://wiki.company.com/runbooks/high-latency"

      - alert: CriticalLatencyP99
        expr: |
          histogram_quantile(0.99, 
            rate(http_server_requests_seconds_bucket{job="spring-boot-app"}[5m])
          ) > 1.0
        for: 2m
        labels:
          severity: critical
          component: api
        annotations:
          summary: "CRITICAL latency p99 on {{ $labels.instance }}"
          description: "p99 latency is {{ $value }}s (threshold: 1s)"
          action: "1. Check slow queries, 2. Scale pods, 3. Enable cache"

      # ==================================================
      # ERROR RATE
      # ==================================================

      - alert: HighErrorRate
        expr: |
          (
            sum(rate(http_server_requests_total{status=~"5..",job="spring-boot-app"}[5m]))
            /
            sum(rate(http_server_requests_total{job="spring-boot-app"}[5m]))
          ) > 0.01
        for: 5m
        labels:
          severity: warning
          component: api
        annotations:
          summary: "High error rate on {{ $labels.instance }}"
          description: "Error rate is {{ $value | humanizePercentage }} (threshold: 1%)"

      - alert: CriticalErrorRate
        expr: |
          (
            sum(rate(http_server_requests_total{status=~"5..",job="spring-boot-app"}[5m]))
            /
            sum(rate(http_server_requests_total{job="spring-boot-app"}[5m]))
          ) > 0.05
        for: 2m
        labels:
          severity: critical
          component: api
        annotations:
          summary: "CRITICAL error rate on {{ $labels.instance }}"
          description: "Error rate is {{ $value | humanizePercentage }} (threshold: 5%)"
          action: "1. Check logs, 2. Rollback if recent deploy, 3. Scale pods"

      # ==================================================
      # SATURATION
      # ==================================================

      - alert: HighCPU
        expr: |
          system_cpu_usage{job="spring-boot-app"} > 0.8
        for: 5m
        labels:
          severity: warning
          component: infrastructure
        annotations:
          summary: "High CPU usage on {{ $labels.instance }}"
          description: "CPU usage is {{ $value | humanizePercentage }} (threshold: 80%)"

      - alert: HighMemory
        expr: |
          (
            jvm_memory_used_bytes{area="heap",job="spring-boot-app"}
            /
            jvm_memory_max_bytes{area="heap",job="spring-boot-app"}
          ) > 0.9
        for: 5m
        labels:
          severity: warning
          component: infrastructure
        annotations:
          summary: "High memory usage on {{ $labels.instance }}"
          description: "Memory usage is {{ $value | humanizePercentage }} (threshold: 90%)"

      - alert: ThreadPoolSaturation
        expr: |
          (
            tomcat_threads_busy_threads{job="spring-boot-app"}
            /
            tomcat_threads_max_threads{job="spring-boot-app"}
          ) > 0.9
        for: 2m
        labels:
          severity: critical
          component: infrastructure
        annotations:
          summary: "Thread pool saturated on {{ $labels.instance }}"
          description: "Thread pool {{ $value | humanizePercentage }} busy (threshold: 90%)"
          action: "Scale pods immediately"

      # ==================================================
      # TRAFFIC
      # ==================================================

      - alert: LowTraffic
        expr: |
          rate(http_server_requests_total{job="spring-boot-app"}[5m]) < 10
        for: 15m
        labels:
          severity: info
          component: api
        annotations:
          summary: "Low traffic on {{ $labels.instance }}"
          description: "RPS is {{ $value }} (expected > 10)"

      - alert: TrafficSpike
        expr: |
          rate(http_server_requests_total{job="spring-boot-app"}[1m])
          /
          avg_over_time(rate(http_server_requests_total{job="spring-boot-app"}[1m])[1h:])
          > 3
        for: 2m
        labels:
          severity: warning
          component: api
        annotations:
          summary: "Traffic spike on {{ $labels.instance }}"
          description: "Traffic is {{ $value }}x normal (3x threshold)"
          action: "Monitor for DDoS, enable rate limiting if needed"
```

---

### 2.2 Multi-Window Alerts (Burn Rate)

```yaml
# alert-rules-burn-rate.yml

groups:
  - name: slo_burn_rate
    interval: 30s
    rules:
      # SLI: Error rate
      - record: sli:error_rate:5m
        expr: |
          sum(rate(http_server_requests_total{status=~"5.."}[5m]))
          /
          sum(rate(http_server_requests_total[5m]))

      - record: sli:error_rate:1h
        expr: |
          sum(rate(http_server_requests_total{status=~"5.."}[1h]))
          /
          sum(rate(http_server_requests_total[1h]))

      # SLO: 99.9% success (0.1% error budget)
      - record: slo:error_budget
        expr: 0.001

      # Burn rate: Velocidade de consumo do error budget
      # burn_rate = 1 â†’ Normal (esgota budget em 30 dias)
      # burn_rate = 10 â†’ CrÃ­tico (esgota budget em 3 dias)

      - alert: FastBurnRate
        expr: |
          sli:error_rate:5m > (10 * slo:error_budget)
          and
          sli:error_rate:1h > (10 * slo:error_budget)
        for: 2m
        labels:
          severity: critical
          slo: availability
        annotations:
          summary: "Fast error budget burn rate"
          description: |
            Error rate is 10x budget:
            - 5min: {{ $value | humanizePercentage }}
            - 1h: {{ $value | humanizePercentage }}
            - Budget: {{ 0.001 | humanizePercentage }}
            At this rate, budget exhausted in 3 days.
          action: "Stop deploys, investigate root cause"

      - alert: ModerateBurnRate
        expr: |
          sli:error_rate:5m > (5 * slo:error_budget)
          and
          sli:error_rate:1h > (5 * slo:error_budget)
        for: 10m
        labels:
          severity: warning
          slo: availability
        annotations:
          summary: "Moderate error budget burn rate"
          description: "Error rate is 5x budget, exhausted in 6 days"
          action: "Extra caution on deploys, monitor closely"
```

---

## 3. Routing e Grouping

### 3.1 Routing Strategies

```yaml
# alertmanager-routing.yml

route:
  receiver: "default"

  # Agrupar por: alertname + cluster
  # Agrupa mÃºltiplas instÃ¢ncias do mesmo alerta
  group_by: ["alertname", "cluster"]

  routes:
    # Database team
    - match_re:
        service: "postgres|mongodb|redis"
      receiver: "team-database"
      group_by: ["alertname", "service"]

    # Frontend team
    - match:
        component: "frontend"
      receiver: "team-frontend"

    # Backend team
    - match:
        component: "backend"
      receiver: "team-backend"

      # Sub-rotas por severidade
      routes:
        - match:
            severity: critical
          receiver: "pagerduty-backend"
          group_wait: 10s # Enviar rÃ¡pido
          repeat_interval: 30m # Re-notificar a cada 30min

        - match:
            severity: warning
          receiver: "slack-backend"
          group_wait: 5m # Agrupar por 5min
          repeat_interval: 4h # Re-notificar a cada 4h
```

---

### 3.2 Time-Based Routing

```yaml
# alertmanager-time-routing.yml

route:
  receiver: "default"
  routes:
    # HorÃ¡rio comercial (09:00-18:00 UTC weekdays)
    - match:
        severity: warning
      receiver: "slack-business-hours"
      active_time_intervals:
        - business_hours

    # Fora do horÃ¡rio comercial
    - match:
        severity: critical
      receiver: "pagerduty-oncall"
      mute_time_intervals:
        - business_hours

# Time intervals
time_intervals:
  - name: business_hours
    time_intervals:
      - times:
          - start_time: "09:00"
            end_time: "18:00"
        weekdays: ["monday:friday"]
        location: "UTC"
```

---

## 4. Inhibition Rules

### 4.1 Conceito

```java
/**
 * INHIBITION: Silenciar alertas secundÃ¡rios quando alerta primÃ¡rio ativo
 *
 * Exemplo:
 * - InstanceDown (primÃ¡rio) ativo
 * - HighLatency, HighErrorRate (secundÃ¡rios) silenciados
 *
 * Motivo: Evitar ruÃ­do (todos alertas sÃ£o consequÃªncia do InstanceDown)
 */
```

```yaml
# alertmanager-inhibition.yml

inhibit_rules:
  # Se instÃ¢ncia down, silencia todos alertas dela
  - source_match:
      alertname: "InstanceDown"
    target_match_re:
      alertname: ".*"
    equal: ["instance"]

  # Se database down, silencia HighErrorRate e HighLatency
  - source_match:
      alertname: "DatabaseDown"
    target_match_re:
      alertname: "HighErrorRate|HighLatency"
    equal: ["service"]

  # Se cluster down, silencia alertas de pods
  - source_match:
      alertname: "ClusterDown"
    target_match_re:
      alertname: "PodDown|PodCrashLooping"
    equal: ["cluster"]

  # Se critical, silencia warning do mesmo tipo
  - source_match:
      severity: "critical"
    target_match:
      severity: "warning"
    equal: ["alertname", "instance"]
```

---

### 4.2 Exemplo PrÃ¡tico

```yaml
# CenÃ¡rio: Database down causa error rate alto

# Alert 1: DatabaseDown (dispara primeiro)
- alert: DatabaseDown
  expr: |
    up{job="postgres"} == 0
  for: 1m
  labels:
    severity: critical
    service: postgres

# Alert 2: HighErrorRate (consequÃªncia)
- alert: HighErrorRate
  expr: |
    rate(http_requests_total{status="500"}[5m]) > 0.1
  for: 5m
  labels:
    severity: critical
    service: backend

# Inhibition: Se DatabaseDown, silencia HighErrorRate
inhibit_rules:
  - source_match:
      alertname: 'DatabaseDown'
    target_match:
      alertname: 'HighErrorRate'
    equal: ['service']  # Mesmo service

# Resultado:
# - Apenas DatabaseDown Ã© notificado
# - HighErrorRate Ã© silenciado (ruÃ­do evitado)
# - Quando DatabaseDown resolve, HighErrorRate pode disparar novamente
```

---

## 5. Integrations

### 5.1 PagerDuty

```yaml
# alertmanager-pagerduty.yml

receivers:
  - name: "pagerduty-critical"
    pagerduty_configs:
      - service_key: "YOUR_SERVICE_KEY"
        description: |
          Alert: {{ .GroupLabels.alertname }}
          Instance: {{ .CommonLabels.instance }}
          Summary: {{ .CommonAnnotations.summary }}
        severity: "{{ .CommonLabels.severity }}"
        details:
          firing: "{{ .Alerts.Firing | len }}"
          resolved: "{{ .Alerts.Resolved | len }}"
        client: "Prometheus AlertManager"
        client_url: "http://alertmanager:9093"
```

```java
/**
 * PagerDuty Events API (manual trigger)
 */
@Service
public class PagerDutyService {

    private static final String PAGERDUTY_API = "https://events.pagerduty.com/v2/enqueue";

    @Autowired
    private RestTemplate restTemplate;

    @Value("${pagerduty.integration-key}")
    private String integrationKey;

    public void triggerIncident(String summary, String source, String severity) {
        var event = Map.of(
            "routing_key", integrationKey,
            "event_action", "trigger",
            "payload", Map.of(
                "summary", summary,
                "source", source,
                "severity", severity,
                "timestamp", Instant.now().toString(),
                "custom_details", Map.of(
                    "environment", "production",
                    "service", "order-service"
                )
            )
        );

        restTemplate.postForEntity(PAGERDUTY_API, event, String.class);
    }
}
```

---

### 5.2 Slack

```yaml
# alertmanager-slack.yml

receivers:
  - name: "slack-critical"
    slack_configs:
      - api_url: "https://hooks.slack.com/services/YOUR/WEBHOOK/URL"
        channel: "#alerts-critical"
        username: "AlertManager"
        icon_emoji: ":fire:"
        title: "ðŸš¨ {{ .GroupLabels.alertname }}"
        text: |
          *Severity:* {{ .CommonLabels.severity }}
          *Instance:* {{ .CommonLabels.instance }}
          *Summary:* {{ .CommonAnnotations.summary }}
          *Description:* {{ .CommonAnnotations.description }}

          *Firing Alerts:*
          {{ range .Alerts.Firing }}
          - {{ .Labels.alertname }}: {{ .Annotations.summary }}
          {{ end }}

          <http://prometheus:9090|Prometheus> | <http://grafana:3000|Grafana>

        send_resolved: true
        title_link: "http://alertmanager:9093"
```

---

### 5.3 Email

```yaml
# alertmanager-email.yml

global:
  smtp_smarthost: "smtp.gmail.com:587"
  smtp_from: "alerts@company.com"
  smtp_auth_username: "alerts@company.com"
  smtp_auth_password: "app-password"
  smtp_require_tls: true

receivers:
  - name: "email-team"
    email_configs:
      - to: "team@company.com"
        headers:
          Subject: "[{{ .Status | toUpper }}] {{ .GroupLabels.alertname }}"
        html: |
          <!DOCTYPE html>
          <html>
          <body>
            <h2 style="color: {{ if eq .Status "firing" }}red{{ else }}green{{ end }}">
              {{ .Status | toUpper }}: {{ .GroupLabels.alertname }}
            </h2>
            
            <h3>Labels:</h3>
            <ul>
              {{ range .CommonLabels.SortedPairs }}
              <li><strong>{{ .Name }}:</strong> {{ .Value }}</li>
              {{ end }}
            </ul>
            
            <h3>Annotations:</h3>
            <ul>
              {{ range .CommonAnnotations.SortedPairs }}
              <li><strong>{{ .Name }}:</strong> {{ .Value }}</li>
              {{ end }}
            </ul>
            
            <h3>Firing Alerts ({{ .Alerts.Firing | len }}):</h3>
            <ul>
              {{ range .Alerts.Firing }}
              <li>{{ .Labels.instance }}: {{ .Annotations.summary }}</li>
              {{ end }}
            </ul>
            
            <p>
              <a href="http://prometheus:9090">Prometheus</a> |
              <a href="http://grafana:3000">Grafana</a> |
              <a href="http://alertmanager:9093">AlertManager</a>
            </p>
          </body>
          </html>
        send_resolved: true
```

---

## ðŸ“Š Checklist de Qualidade

- [ ] AlertManager integrado ao Prometheus
- [ ] Routing configurado por severidade (critical/warning/info)
- [ ] Grouping por alertname + cluster/service
- [ ] Inhibition rules para evitar ruÃ­do (instance down silencia outros)
- [ ] Multi-window alerts (burn rate) para SLOs
- [ ] PagerDuty/Opsgenie para critical alerts (on-call)
- [ ] Slack para warning/info alerts (visibilidade)
- [ ] Email como fallback
- [ ] Templates customizados para mensagens claras
- [ ] Runbooks linkados nas annotations

---

## ðŸŽ¯ ExercÃ­cios PrÃ¡ticos

1. **BÃ¡sico**: Configurar AlertManager com Slack integration
2. **IntermediÃ¡rio**: Criar burn rate alerts (multi-window)
3. **AvanÃ§ado**: Implementar inhibition rules complexas (cascading failures)

---

## ðŸ“š ReferÃªncias

- [Prometheus Alerting](https://prometheus.io/docs/alerting/latest/overview/)
- [AlertManager Configuration](https://prometheus.io/docs/alerting/latest/configuration/)
- [PromQL Cheat Sheet](https://promlabs.com/promql-cheat-sheet/)
- [AlertManager Best Practices](https://prometheus.io/docs/practices/alerting/)

---

**Anterior:** [13.1 - Fundamentos de Alertas](13.1-fundamentos-alertas.md)  
**PrÃ³ximo:** [13.3 - Thresholds DinÃ¢micos](13.3-thresholds-dinamicos.md)
