# üß™ M√≥dulo 13.5: Testing Alerts

> **Objetivo:** Validar alertas com testes automatizados, chaos engineering e Testcontainers.

---

## üìë √çndice

1. [Alert Testing Strategy](#1-alert-testing-strategy)
2. [Testcontainers Prometheus](#2-testcontainers-prometheus)
3. [Chaos Engineering](#3-chaos-engineering)
4. [Alert Simulation](#4-alert-simulation)
5. [Observability Tests](#5-observability-tests)

---

## 1. Alert Testing Strategy

### 1.1 Tipos de Testes

```java
/**
 * PIR√ÇMIDE DE TESTES DE ALERTAS
 *
 *        /\
 *       /  \  E2E (Chaos Engineering)
 *      /____\
 *     /      \ Integration (Alert fires em ambiente real)
 *    /________\
 *   /          \ Unit (PromQL queries, thresholds)
 *  /____________\
 *
 * Unit Tests:
 * - Validar sintaxe PromQL
 * - Testar l√≥gica de threshold
 * - Mock de m√©tricas
 *
 * Integration Tests:
 * - Prometheus + AlertManager reais (Testcontainers)
 * - Simular m√©tricas que disparam alertas
 * - Validar routing, grouping, inhibition
 *
 * E2E Tests (Chaos):
 * - Injetar falhas reais (kill pods, saturar CPU)
 * - Validar que alertas disparam
 * - Validar runbooks funcionam
 */
```

---

### 1.2 Alert Testing Checklist

```java
/**
 * Checklist para cada alerta
 */
@TestInstance(TestInstance.Lifecycle.PER_CLASS)
class AlertTest {

    /**
     * ‚úÖ 1. Sintaxe PromQL v√°lida
     */
    @Test
    void testPromQLSyntax() {
        var query = "rate(http_requests_total{status=~\"5..\"}[5m]) > 0.05";

        assertThat(PromQLValidator.isValid(query)).isTrue();
    }

    /**
     * ‚úÖ 2. Threshold calibrado (false positive < 5%)
     */
    @Test
    void testThresholdCalibration() {
        var historicalData = loadHistoricalMetrics("latency_p99", 30);
        var threshold = 0.5;  // 500ms

        var falsePositiveRate = calculateFalsePositives(historicalData, threshold);

        assertThat(falsePositiveRate).isLessThan(5.0);
    }

    /**
     * ‚úÖ 3. Alert dispara quando deveria
     */
    @Test
    void testAlertFires() {
        // Simular m√©trica acima do threshold
        metricsSimulator.push("latency_p99", 0.6);  // 600ms

        await().atMost(Duration.ofMinutes(6))
               .until(() -> alertManager.getFiringAlerts().stream()
                                        .anyMatch(a -> a.getName().equals("HighLatency")));
    }

    /**
     * ‚úÖ 4. Alert N√ÉO dispara quando n√£o deveria
     */
    @Test
    void testAlertDoesNotFire() {
        metricsSimulator.push("latency_p99", 0.4);  // 400ms (abaixo threshold)

        Thread.sleep(Duration.ofMinutes(6).toMillis());

        var firingAlerts = alertManager.getFiringAlerts();
        assertThat(firingAlerts).noneMatch(a -> a.getName().equals("HighLatency"));
    }

    /**
     * ‚úÖ 5. Runbook existe e est√° atualizado
     */
    @Test
    void testRunbookExists() {
        var alert = loadAlert("HighLatency");
        var runbookUrl = alert.getLabels().get("runbook");

        assertThat(runbookUrl).isNotNull();
        assertThat(HttpClient.get(runbookUrl).getStatusCode()).isEqualTo(200);
    }

    /**
     * ‚úÖ 6. Routing correto (PagerDuty para critical)
     */
    @Test
    void testAlertRouting() {
        metricsSimulator.push("error_rate", 0.1);  // 10% errors (critical)

        await().atMost(Duration.ofMinutes(3))
               .until(() -> pagerDutyMock.getIncidents().stream()
                                         .anyMatch(i -> i.getTitle().contains("HighErrorRate")));
    }

    /**
     * ‚úÖ 7. Inhibition rules funcionam
     */
    @Test
    void testInhibitionRules() {
        // Disparar InstanceDown (prim√°rio)
        metricsSimulator.push("up", 0);

        await().atMost(Duration.ofMinutes(2))
               .until(() -> alertManager.getFiringAlerts().stream()
                                        .anyMatch(a -> a.getName().equals("InstanceDown")));

        // Disparar HighLatency (secund√°rio, deve ser inibido)
        metricsSimulator.push("latency_p99", 2.0);

        Thread.sleep(Duration.ofMinutes(6).toMillis());

        // HighLatency N√ÉO deve estar firing (inibido por InstanceDown)
        var firingAlerts = alertManager.getFiringAlerts();
        assertThat(firingAlerts).noneMatch(a -> a.getName().equals("HighLatency"));
    }
}
```

---

## 2. Testcontainers Prometheus

### 2.1 Setup

```java
/**
 * Testcontainers: Prometheus + AlertManager
 */
@SpringBootTest
@Testcontainers
class PrometheusAlertTest {

    @Container
    static GenericContainer<?> prometheus = new GenericContainer<>("prom/prometheus:v2.48.0")
            .withExposedPorts(9090)
            .withFileSystemBind("./prometheus.yml", "/etc/prometheus/prometheus.yml")
            .withFileSystemBind("./alert-rules.yml", "/etc/prometheus/alert-rules.yml")
            .withCommand("--config.file=/etc/prometheus/prometheus.yml");

    @Container
    static GenericContainer<?> alertmanager = new GenericContainer<>("prom/alertmanager:v0.26.0")
            .withExposedPorts(9093)
            .withFileSystemBind("./alertmanager.yml", "/etc/alertmanager/alertmanager.yml")
            .withCommand("--config.file=/etc/alertmanager/alertmanager.yml");

    @DynamicPropertySource
    static void configureProperties(DynamicPropertyRegistry registry) {
        registry.add("prometheus.url", () ->
            "http://localhost:" + prometheus.getMappedPort(9090));

        registry.add("alertmanager.url", () ->
            "http://localhost:" + alertmanager.getMappedPort(9093));
    }

    @Autowired
    private RestTemplate restTemplate;

    private String prometheusUrl;
    private String alertmanagerUrl;

    @BeforeEach
    void setup() {
        prometheusUrl = "http://localhost:" + prometheus.getMappedPort(9090);
        alertmanagerUrl = "http://localhost:" + alertmanager.getMappedPort(9093);
    }

    /**
     * Push m√©tricas para Prometheus (Pushgateway)
     */
    private void pushMetric(String metric, double value) {
        var pushgatewayUrl = prometheusUrl.replace("9090", "9091");
        var body = String.format("%s %f\n", metric, value);

        restTemplate.postForEntity(
            pushgatewayUrl + "/metrics/job/test",
            body,
            String.class
        );
    }

    /**
     * Query alertas firing
     */
    private List<Alert> getFiringAlerts() {
        var url = alertmanagerUrl + "/api/v2/alerts?filter=alertstate=\"firing\"";
        var response = restTemplate.getForEntity(url, AlertsResponse.class);

        return response.getBody().getAlerts();
    }
}
```

---

### 2.2 Test com Pushgateway

```java
/**
 * Test: Alerta dispara quando m√©trica excede threshold
 */
@Test
void testHighLatencyAlertFires() {
    // Push m√©trica acima do threshold (500ms)
    pushMetric("http_request_duration_seconds_sum", 10.0);   // 10s total
    pushMetric("http_request_duration_seconds_count", 15);   // 15 requests
    // M√©dia: 10s / 15 = 666ms > 500ms threshold

    // Esperar alerta disparar (5min for + 1min evaluation)
    await().atMost(Duration.ofMinutes(7))
           .pollInterval(Duration.ofSeconds(10))
           .until(() -> {
               var alerts = getFiringAlerts();
               return alerts.stream()
                            .anyMatch(a -> a.getLabels().get("alertname").equals("HighLatency"));
           });

    // Validar labels e annotations
    var alert = getFiringAlerts().stream()
                                 .filter(a -> a.getLabels().get("alertname").equals("HighLatency"))
                                 .findFirst()
                                 .orElseThrow();

    assertThat(alert.getLabels().get("severity")).isEqualTo("warning");
    assertThat(alert.getAnnotations().get("summary")).contains("High latency");
}

/**
 * Test: Alerta resolve quando m√©trica normaliza
 */
@Test
void testHighLatencyAlertResolves() {
    // 1. Disparar alerta
    pushMetric("http_request_duration_seconds_sum", 10.0);
    pushMetric("http_request_duration_seconds_count", 15);

    await().until(() -> getFiringAlerts().stream()
                                         .anyMatch(a -> a.getLabels().get("alertname").equals("HighLatency")));

    // 2. Normalizar m√©trica (abaixo threshold)
    pushMetric("http_request_duration_seconds_sum", 5.0);   // 5s total
    pushMetric("http_request_duration_seconds_count", 15);  // 15 requests
    // M√©dia: 5s / 15 = 333ms < 500ms threshold

    // 3. Esperar alerta resolver (5min resolve_timeout)
    await().atMost(Duration.ofMinutes(7))
           .until(() -> getFiringAlerts().stream()
                                         .noneMatch(a -> a.getLabels().get("alertname").equals("HighLatency")));
}
```

---

## 3. Chaos Engineering

### 3.1 Chaos Toolkit

```yaml
# chaos-experiment.yml

version: 1.0.0
title: "Validate alerts during database outage"
description: |
  Kill database pod and validate:
  1. DatabaseDown alert fires
  2. HighErrorRate alert fires (but inhibited by DatabaseDown)
  3. Circuit breaker opens automatically

steady-state-hypothesis:
  title: "System is healthy"
  probes:
    - name: "database-is-up"
      type: probe
      provider:
        type: http
        url: "http://app:8080/actuator/health/db"
        timeout: 5
      tolerance: 200

    - name: "no-critical-alerts"
      type: probe
      provider:
        type: http
        url: 'http://alertmanager:9093/api/v2/alerts?filter=severity="critical"'
      tolerance:
        - status: 200
        - body:
            alerts: []

method:
  - type: action
    name: "kill-database-pod"
    provider:
      type: process
      path: "kubectl"
      arguments: ["delete", "pod", "-l", "app=postgres"]

  - type: probe
    name: "wait-for-alert"
    provider:
      type: http
      url: 'http://alertmanager:9093/api/v2/alerts?filter=alertname="DatabaseDown"'
      timeout: 300
    tolerance:
      - status: 200
      - body:
          alerts:
            - labels:
                alertname: "DatabaseDown"
                severity: "critical"

rollbacks:
  - type: action
    name: "scale-database-back-up"
    provider:
      type: process
      path: "kubectl"
      arguments: ["scale", "statefulset", "postgres", "--replicas=1"]
```

```bash
# Executar experimento
chaos run chaos-experiment.yml
```

---

### 3.2 Chaos Mesh (Kubernetes)

```yaml
# chaos-mesh-experiment.yml

apiVersion: chaos-mesh.org/v1alpha1
kind: PodChaos
metadata:
  name: database-kill
spec:
  action: pod-kill
  mode: one
  selector:
    namespaces:
      - default
    labelSelectors:
      app: postgres
  duration: "5m"
  scheduler:
    cron: "@every 1h" # Executar a cada hora

---
apiVersion: chaos-mesh.org/v1alpha1
kind: NetworkChaos
metadata:
  name: database-delay
spec:
  action: delay
  mode: one
  selector:
    namespaces:
      - default
    labelSelectors:
      app: postgres
  delay:
    latency: "1s"
    correlation: "100"
    jitter: "0ms"
  duration: "10m"
```

```java
/**
 * Test: Validar alertas durante chaos
 */
@Test
@ChaosExperiment("database-kill")
void testAlertsDuringDatabaseOutage() {
    // 1. Estado inicial: Database saud√°vel
    assertThat(healthCheck.isDatabaseUp()).isTrue();
    assertThat(alertManager.getCriticalAlerts()).isEmpty();

    // 2. Chaos: Kill database pod (via annotation @ChaosExperiment)
    // Chaos Mesh executar√° automaticamente

    // 3. Validar alertas
    await().atMost(Duration.ofMinutes(5))
           .until(() -> alertManager.getFiringAlerts().stream()
                                    .anyMatch(a -> a.getName().equals("DatabaseDown")));

    // 4. Validar circuit breaker abriu
    await().atMost(Duration.ofSeconds(30))
           .until(() -> circuitBreakerRegistry.circuitBreaker("database")
                                             .getState() == CircuitBreaker.State.OPEN);

    // 5. Validar error rate sobe (mas alerta inibido)
    await().atMost(Duration.ofMinutes(2))
           .until(() -> {
               var errorRate = getErrorRate();
               return errorRate > 0.01;  // > 1%
           });

    // 6. Validar HighErrorRate N√ÉO dispara (inibido por DatabaseDown)
    Thread.sleep(Duration.ofMinutes(7).toMillis());
    var firingAlerts = alertManager.getFiringAlerts();
    assertThat(firingAlerts).noneMatch(a -> a.getName().equals("HighErrorRate"));

    // 7. Rollback (Chaos Mesh para experimento automaticamente ap√≥s duration)
    await().atMost(Duration.ofMinutes(10))
           .until(() -> healthCheck.isDatabaseUp());

    // 8. Validar alertas resolvem
    await().atMost(Duration.ofMinutes(7))
           .until(() -> alertManager.getFiringAlerts().isEmpty());
}
```

---

## 4. Alert Simulation

### 4.1 Mock Metrics Exporter

```java
/**
 * Mock exporter: Simular m√©tricas para testes
 */
@Service
public class MockMetricsExporter {

    @Autowired
    private MeterRegistry registry;

    private final Map<String, Gauge> gauges = new ConcurrentHashMap<>();

    /**
     * Simular m√©trica
     */
    public void setMetric(String name, double value) {
        gauges.computeIfAbsent(name, n ->
            Gauge.builder(n, () -> value)
                 .register(registry)
        );
    }

    /**
     * Simular lat√™ncia alta
     */
    public void simulateHighLatency(Duration duration) {
        setMetric("http_request_duration_seconds", duration.toMillis() / 1000.0);
    }

    /**
     * Simular error rate alto
     */
    public void simulateHighErrorRate(double rate) {
        setMetric("http_requests_total", 100.0);
        setMetric("http_requests_errors", rate * 100);
    }

    /**
     * Simular CPU alto
     */
    public void simulateHighCPU(double percentage) {
        setMetric("system_cpu_usage", percentage);
    }
}
```

---

### 4.2 Integration Test

```java
/**
 * Test: Simular cen√°rios e validar alertas
 */
@SpringBootTest
@Testcontainers
class AlertIntegrationTest {

    @Autowired
    private MockMetricsExporter metricsExporter;

    @Autowired
    private AlertManagerClient alertManager;

    /**
     * Cen√°rio 1: Lat√™ncia alta ‚Üí Alert warning
     */
    @Test
    void testHighLatencyScenario() {
        metricsExporter.simulateHighLatency(Duration.ofMillis(600));

        await().atMost(Duration.ofMinutes(6))
               .until(() -> alertManager.hasAlert("HighLatency", Severity.WARNING));
    }

    /**
     * Cen√°rio 2: Lat√™ncia cr√≠tica ‚Üí Alert critical
     */
    @Test
    void testCriticalLatencyScenario() {
        metricsExporter.simulateHighLatency(Duration.ofMillis(1200));

        await().atMost(Duration.ofMinutes(3))
               .until(() -> alertManager.hasAlert("CriticalLatency", Severity.CRITICAL));
    }

    /**
     * Cen√°rio 3: Traffic spike ‚Üí Warning
     */
    @Test
    void testTrafficSpikeScenario() {
        var baselineRps = 100.0;
        var spikeRps = baselineRps * 4;  // 4x baseline

        metricsExporter.setMetric("http_requests_total", spikeRps);

        await().atMost(Duration.ofMinutes(3))
               .until(() -> alertManager.hasAlert("TrafficSpike", Severity.WARNING));
    }

    /**
     * Cen√°rio 4: Cascading failure (Database ‚Üí Errors ‚Üí Latency)
     */
    @Test
    void testCascadingFailureScenario() {
        // 1. Database down
        metricsExporter.setMetric("up{job=\"postgres\"}", 0.0);

        await().until(() -> alertManager.hasAlert("DatabaseDown", Severity.CRITICAL));

        // 2. Error rate sobe (consequ√™ncia)
        metricsExporter.simulateHighErrorRate(0.15);  // 15%

        // 3. HighErrorRate N√ÉO dispara (inibido)
        Thread.sleep(Duration.ofMinutes(7).toMillis());
        assertThat(alertManager.hasAlert("HighErrorRate", Severity.CRITICAL)).isFalse();

        // 4. Lat√™ncia sobe (consequ√™ncia)
        metricsExporter.simulateHighLatency(Duration.ofSeconds(2));

        // 5. HighLatency N√ÉO dispara (inibido)
        Thread.sleep(Duration.ofMinutes(7).toMillis());
        assertThat(alertManager.hasAlert("HighLatency", Severity.WARNING)).isFalse();

        // 6. Apenas DatabaseDown firing
        var firingAlerts = alertManager.getFiringAlerts();
        assertThat(firingAlerts).hasSize(1);
        assertThat(firingAlerts.get(0).getName()).isEqualTo("DatabaseDown");
    }
}
```

---

## 5. Observability Tests

### 5.1 Contract Testing (Metrics)

```java
/**
 * Contract test: Validar que m√©tricas existem
 */
@SpringBootTest(webEnvironment = SpringBootTest.WebEnvironment.RANDOM_PORT)
class MetricsContractTest {

    @Autowired
    private TestRestTemplate restTemplate;

    @Test
    void testPrometheusEndpointExists() {
        var response = restTemplate.getForEntity("/actuator/prometheus", String.class);

        assertThat(response.getStatusCode()).isEqualTo(HttpStatus.OK);
    }

    @Test
    void testRequiredMetricsExist() {
        var metrics = restTemplate.getForObject("/actuator/prometheus", String.class);

        // Golden Signals
        assertThat(metrics).contains("http_server_requests_seconds");  // Latency
        assertThat(metrics).contains("http_requests_total");           // Traffic
        assertThat(metrics).contains("system_cpu_usage");              // Saturation

        // Business metrics
        assertThat(metrics).contains("orders_created_total");
        assertThat(metrics).contains("payment_processed_total");
    }

    @Test
    void testMetricsHaveRequiredLabels() {
        var metrics = restTemplate.getForObject("/actuator/prometheus", String.class);

        // http_requests_total deve ter labels: method, endpoint, status
        assertThat(metrics).containsPattern(
            "http_requests_total\\{.*method=\".*\",.*endpoint=\".*\",.*status=\".*\".*\\}"
        );
    }
}
```

---

### 5.2 SLO Testing

```java
/**
 * Test: Validar se SLO est√° sendo atingido
 */
@SpringBootTest
class SloTest {

    @Autowired
    private PrometheusClient prometheus;

    /**
     * SLO: Lat√™ncia p99 < 500ms em 99% do tempo (√∫ltimos 30 dias)
     */
    @Test
    void testLatencySlo() {
        var query = """
            (
              histogram_quantile(0.99,
                rate(http_server_requests_seconds_bucket[5m])
              ) < 0.5
            ) * 100
            """;

        var compliance = prometheus.queryRange(query, Duration.ofDays(30))
                                   .stream()
                                   .mapToDouble(Point::getValue)
                                   .average()
                                   .orElse(0);

        assertThat(compliance).isGreaterThan(99.0);
    }

    /**
     * SLO: Error rate < 0.1% (99.9% success)
     */
    @Test
    void testErrorRateSlo() {
        var query = """
            (
              sum(rate(http_requests_total{status=~"5.."}[5m]))
              /
              sum(rate(http_requests_total[5m]))
            ) < 0.001
            """;

        var compliance = prometheus.queryRange(query, Duration.ofDays(30))
                                   .stream()
                                   .filter(p -> p.getValue() < 0.001)
                                   .count();

        var totalPoints = prometheus.queryRange(query, Duration.ofDays(30)).size();
        var compliancePercentage = (double) compliance / totalPoints * 100;

        assertThat(compliancePercentage).isGreaterThan(99.9);
    }
}
```

---

## üìä Checklist de Qualidade

- [ ] Todos alertas t√™m testes automatizados
- [ ] Testcontainers com Prometheus + AlertManager
- [ ] Chaos experiments executados regularmente
- [ ] Alert simulation em CI/CD
- [ ] Contract tests para m√©tricas cr√≠ticas
- [ ] SLO compliance validado em testes
- [ ] Runbooks testados (manuais ou automatizados)
- [ ] False positive rate medido (< 5%)
- [ ] MTTA/MTTR rastreados em dashboards
- [ ] Post-mortems para incidents SEV-1

---

## üéØ Exerc√≠cios Pr√°ticos

1. **B√°sico**: Criar test que valida alerta dispara em threshold
2. **Intermedi√°rio**: Setup Testcontainers com Prometheus + AlertManager
3. **Avan√ßado**: Chaos experiment com kill pod + valida√ß√£o de alertas

---

## üìö Refer√™ncias

- [Prometheus Testing](https://prometheus.io/docs/prometheus/latest/configuration/unit_testing_rules/)
- [Chaos Toolkit](https://chaostoolkit.org/)
- [Chaos Mesh](https://chaos-mesh.org/)
- [Testcontainers](https://www.testcontainers.org/)

---

**Anterior:** [13.4 - Alert Fatigue](13.4-alert-fatigue.md)  
**Pr√≥ximo:** Fase 14 ou revis√£o
