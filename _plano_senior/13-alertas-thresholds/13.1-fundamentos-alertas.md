# üö® M√≥dulo 13.1: Fundamentos de Alertas e SLOs

> **Objetivo:** Dominar SLI/SLO/SLA, Error Budget e Golden Signals para alertas eficazes.

---

## üìë √çndice

1. [SLI, SLO e SLA](#1-sli-slo-e-sla)
2. [Error Budget](#2-error-budget)
3. [Golden Signals](#3-golden-signals)
4. [Alert Design](#4-alert-design)
5. [Prioriza√ß√£o](#5-priorizacao)

---

## 1. SLI, SLO e SLA

### 1.1 Defini√ß√µes

```java
/**
 * SLI (Service Level Indicator): M√©trica quantitativa
 *
 * Exemplos:
 * - Lat√™ncia p99 das requisi√ß√µes HTTP
 * - Taxa de erro (5xx / total requests)
 * - Disponibilidade (uptime / total time)
 */

/**
 * SLO (Service Level Objective): Meta interna
 *
 * Exemplos:
 * - Lat√™ncia p99 < 500ms (99% do tempo)
 * - Taxa de erro < 0.1% (99.9% de sucesso)
 * - Disponibilidade >= 99.9% (43min downtime/m√™s)
 */

/**
 * SLA (Service Level Agreement): Contrato externo
 *
 * Exemplos:
 * - Disponibilidade >= 99.95% ou cr√©dito de 10%
 * - Lat√™ncia p95 < 1s ou SLA breach
 */

/**
 * HIERARQUIA:
 *
 * SLA (mais relaxado, contrato)
 *  ‚Üë
 * SLO (mais restrito, meta interna)
 *  ‚Üë
 * SLI (medi√ß√£o real)
 *
 * Exemplo:
 * - SLA: 99.9% uptime (contrato com cliente)
 * - SLO: 99.95% uptime (meta interna, buffer de seguran√ßa)
 * - SLI: 99.97% uptime (medi√ß√£o atual)
 */
```

---

### 1.2 Implementa√ß√£o de SLI

```java
/**
 * SLI: Lat√™ncia p99 das requisi√ß√µes HTTP
 */
@Service
public class LatencySliService {

    @Autowired
    private MeterRegistry meterRegistry;

    /**
     * Registrar lat√™ncia de cada request
     */
    @Around("@annotation(org.springframework.web.bind.annotation.RequestMapping)")
    public Object measureLatency(ProceedingJoinPoint joinPoint) throws Throwable {
        var timer = Timer.start(meterRegistry);

        try {
            return joinPoint.proceed();
        } finally {
            timer.stop(Timer.builder("http.server.requests")
                            .tag("method", getMethod(joinPoint))
                            .tag("endpoint", getEndpoint(joinPoint))
                            .register(meterRegistry));
        }
    }

    /**
     * Query PromQL: Calcular p99 dos √∫ltimos 5 minutos
     *
     * histogram_quantile(0.99,
     *   rate(http_server_requests_seconds_bucket[5m])
     * )
     */
}

/**
 * SLI: Taxa de erro (5xx)
 */
@Component
public class ErrorRateSliService {

    @Autowired
    private Counter successCounter;

    @Autowired
    private Counter errorCounter;

    @EventListener
    public void onRequestCompleted(HttpRequestCompletedEvent event) {
        if (event.getStatusCode() >= 500) {
            errorCounter.increment();
        } else {
            successCounter.increment();
        }
    }

    /**
     * Query PromQL: Taxa de erro dos √∫ltimos 5 minutos
     *
     * sum(rate(http_requests_total{status=~"5.."}[5m]))
     * /
     * sum(rate(http_requests_total[5m]))
     */
}
```

---

### 1.3 SLO Window-Based

```yaml
# prometheus-rules.yml

groups:
  - name: slo_latency
    interval: 30s
    rules:
      # SLI: Lat√™ncia p99
      - record: sli:http_request_duration_seconds:p99
        expr: |
          histogram_quantile(0.99, 
            rate(http_server_requests_seconds_bucket[5m])
          )

      # SLO: p99 < 500ms (0.5s)
      - record: slo:latency:target
        expr: 0.5

      # Compliance: % de tempo dentro do SLO
      - record: slo:latency:compliance
        expr: |
          (
            sli:http_request_duration_seconds:p99 < bool slo:latency:target
          ) * 100

      # Alert se compliance < 99% (violando SLO)
      - alert: LatencySloViolation
        expr: |
          slo:latency:compliance < 99
        for: 5m
        labels:
          severity: critical
          slo: latency_p99
        annotations:
          summary: "Lat√™ncia p99 violando SLO (< 99% compliance)"
          description: "p99 = {{ $value }}s (target < 0.5s)"
```

---

## 2. Error Budget

### 2.1 Conceito

```java
/**
 * ERROR BUDGET: "Or√ßamento" de downtime/erros permitido
 *
 * F√≥rmula:
 * Error Budget = 100% - SLO
 *
 * Exemplo:
 * - SLO: 99.9% uptime
 * - Error Budget: 0.1% (43 minutos/m√™s de downtime)
 *
 * USOS:
 * - Decidir velocidade de releases (consumir budget)
 * - Priorizar confiabilidade vs features
 * - Justificar pausa em deploys
 */

/**
 * ESTADOS:
 *
 * Budget > 50%: Deploy liberado, priorizar features
 * Budget 20-50%: Cautela, revisar PRs, testes extras
 * Budget < 20%: FREEZE de features, foco em confiabilidade
 * Budget esgotado: STOP de deploys at√© pr√≥ximo ciclo
 */
```

---

### 2.2 C√°lculo de Error Budget

```java
/**
 * Error Budget Calculator
 */
@Service
public class ErrorBudgetService {

    @Autowired
    private PrometheusClient prometheus;

    /**
     * Calcular error budget consumido
     *
     * @param sloTarget SLO em % (ex: 99.9)
     * @param windowDays Janela de tempo (ex: 30 dias)
     * @return Percentual do budget consumido
     */
    public ErrorBudgetStatus calculateErrorBudget(double sloTarget, int windowDays) {
        // 1. Query uptime real (Prometheus)
        var uptimeQuery = String.format(
            "avg_over_time(up[%dd])", windowDays
        );
        var actualUptime = prometheus.query(uptimeQuery).asDouble() * 100;

        // 2. Error budget dispon√≠vel
        var errorBudget = 100.0 - sloTarget;

        // 3. Error budget consumido
        var actualErrors = 100.0 - actualUptime;
        var budgetConsumed = (actualErrors / errorBudget) * 100;

        // 4. Downtime permitido vs real
        var minutesPerWindow = windowDays * 24 * 60;
        var allowedDowntime = minutesPerWindow * (errorBudget / 100);
        var actualDowntime = minutesPerWindow * (actualErrors / 100);

        return new ErrorBudgetStatus(
            sloTarget,
            actualUptime,
            errorBudget,
            budgetConsumed,
            allowedDowntime,
            actualDowntime,
            determinePolicy(budgetConsumed)
        );
    }

    private DeployPolicy determinePolicy(double budgetConsumed) {
        if (budgetConsumed < 50) {
            return DeployPolicy.NORMAL;
        } else if (budgetConsumed < 80) {
            return DeployPolicy.CAUTIOUS;
        } else if (budgetConsumed < 100) {
            return DeployPolicy.FREEZE;
        } else {
            return DeployPolicy.BLOCKED;
        }
    }
}

record ErrorBudgetStatus(
    double sloTarget,           // 99.9
    double actualUptime,        // 99.85
    double errorBudget,         // 0.1
    double budgetConsumed,      // 50% (metade do budget usado)
    double allowedDowntime,     // 43.2 minutos
    double actualDowntime,      // 21.6 minutos
    DeployPolicy policy         // NORMAL / CAUTIOUS / FREEZE / BLOCKED
) {}

enum DeployPolicy {
    NORMAL,    // Budget > 50%: Deploy normal
    CAUTIOUS,  // Budget 50-80%: Extra review
    FREEZE,    // Budget 80-100%: Freeze de features
    BLOCKED    // Budget esgotado: STOP
}
```

---

### 2.3 Dashboard de Error Budget

```yaml
# Prometheus queries para dashboard

# 1. Uptime atual (√∫ltimos 30 dias)
avg_over_time(up[30d]) * 100

# 2. Error budget consumido (%)
(
  (100 - avg_over_time(up[30d]) * 100)
  /
  (100 - 99.9)
) * 100

# 3. Downtime restante (minutos)
(
  (30 * 24 * 60) * ((100 - 99.9) / 100)
)
-
(
  (30 * 24 * 60) * ((100 - avg_over_time(up[30d]) * 100) / 100)
)

# 4. Burn rate (velocidade de consumo do budget)
# Budget consumido nas √∫ltimas 24h vs m√©dia de 30 dias
(
  (100 - avg_over_time(up[1d]) * 100) / (100 - 99.9)
)
/
(
  (100 - avg_over_time(up[30d]) * 100) / (100 - 99.9)
)
# > 1 = Consumindo mais r√°pido que m√©dia
# < 1 = Consumindo mais lento
```

---

## 3. Golden Signals

### 3.1 Os 4 Pilares (Google SRE)

```java
/**
 * GOLDEN SIGNALS: 4 m√©tricas essenciais
 *
 * 1. LATENCY: Tempo de resposta
 * 2. TRAFFIC: Volume de requisi√ß√µes
 * 3. ERRORS: Taxa de falhas
 * 4. SATURATION: Uso de recursos (CPU, mem√≥ria, disk)
 *
 * REGRA: Monitorar TODAS essas m√©tricas
 */

/**
 * 1. LATENCY
 */
@Configuration
public class LatencyMetrics {

    @Bean
    public TimedAspect timedAspect(MeterRegistry registry) {
        return new TimedAspect(registry);
    }
}

@RestController
public class OrderController {

    @GetMapping("/orders/{id}")
    @Timed(value = "http.get.order", percentiles = {0.5, 0.95, 0.99})
    public Order getOrder(@PathVariable Long id) {
        // Micrometer registra lat√™ncia automaticamente
        return orderService.findById(id);
    }
}

/**
 * PromQL: Lat√™ncia por percentil
 *
 * # p50, p95, p99
 * histogram_quantile(0.50, rate(http_get_order_seconds_bucket[5m]))
 * histogram_quantile(0.95, rate(http_get_order_seconds_bucket[5m]))
 * histogram_quantile(0.99, rate(http_get_order_seconds_bucket[5m]))
 */

/**
 * 2. TRAFFIC
 */
@Component
public class TrafficMetrics {

    @Autowired
    private Counter requestCounter;

    @EventListener
    public void onRequest(HttpRequestEvent event) {
        requestCounter.increment();
    }
}

/**
 * PromQL: Requisi√ß√µes por segundo (RPS)
 *
 * rate(http_requests_total[1m])
 */

/**
 * 3. ERRORS
 */
@ControllerAdvice
public class ErrorMetrics {

    @Autowired
    private Counter errorCounter;

    @ExceptionHandler(Exception.class)
    public ResponseEntity<ErrorResponse> handleError(Exception ex) {
        errorCounter.increment(
            Tags.of("type", ex.getClass().getSimpleName())
        );

        return ResponseEntity.status(500).body(new ErrorResponse(ex));
    }
}

/**
 * PromQL: Taxa de erro (%)
 *
 * sum(rate(http_requests_total{status=~"5.."}[5m]))
 * /
 * sum(rate(http_requests_total[5m]))
 * * 100
 */

/**
 * 4. SATURATION
 */
@Component
public class SaturationMetrics {

    @Scheduled(fixedRate = 10000)
    public void recordSystemMetrics(MeterRegistry registry) {
        // CPU
        registry.gauge("system.cpu.usage",
            getSystemCpuUsage());

        // Memory
        registry.gauge("jvm.memory.used",
            Runtime.getRuntime().totalMemory() - Runtime.getRuntime().freeMemory());

        // Thread pool
        registry.gauge("tomcat.threads.busy",
            getTomcatBusyThreads());
    }
}

/**
 * PromQL: Satura√ß√£o (%)
 *
 * # CPU
 * system_cpu_usage * 100
 *
 * # Memory
 * (jvm_memory_used_bytes / jvm_memory_max_bytes) * 100
 *
 * # Thread pool
 * (tomcat_threads_busy / tomcat_threads_max) * 100
 */
```

---

### 3.2 Thresholds para Golden Signals

| M√©trica               | Threshold Warning | Threshold Critical | Janela |
| --------------------- | ----------------- | ------------------ | ------ |
| **Latency p99**       | > 500ms           | > 1s               | 5 min  |
| **Latency p95**       | > 300ms           | > 700ms            | 5 min  |
| **Traffic (RPS)**     | < 10% baseline    | < 5% baseline      | 5 min  |
| **Error Rate**        | > 1%              | > 5%               | 5 min  |
| **CPU Saturation**    | > 70%             | > 90%              | 5 min  |
| **Memory Saturation** | > 80%             | > 95%              | 1 min  |
| **Thread Pool**       | > 80%             | > 95%              | 1 min  |

---

## 4. Alert Design

### 4.1 Princ√≠pios de Alertas Eficazes

```java
/**
 * PRINC√çPIOS DE ALERTAS (Google SRE)
 *
 * 1. ACTIONABLE: Alerta deve ter a√ß√£o clara
 *    ‚ùå "CPU alto" (o que fazer?)
 *    ‚úÖ "CPU > 90%, escalar pods" (a√ß√£o clara)
 *
 * 2. SYMPTOMATIC: Alerta sobre sintomas (usu√°rio afetado)
 *    ‚ùå "Disk 80% cheio" (usu√°rio n√£o afetado ainda)
 *    ‚úÖ "Lat√™ncia p99 > 1s" (usu√°rio sente lentid√£o)
 *
 * 3. TIMELY: Alerta antes de usu√°rio reportar
 *    - Error budget: Alerta quando budget < 20%
 *    - Burn rate: Alerta se consumindo budget 10x mais r√°pido
 *
 * 4. RELEVANT: Sem falsos positivos (< 5%)
 *    - Usar `for: 5m` (esperar 5min antes de disparar)
 *    - Evitar thresholds muito sens√≠veis
 */
```

---

### 4.2 Alert Taxonomy

```yaml
# N√≠veis de severidade

# CRITICAL: Impacto imediato em usu√°rios, requer a√ß√£o AGORA
- alert: HighErrorRate
  expr: |
    (
      sum(rate(http_requests_total{status=~"5.."}[5m])) 
      / 
      sum(rate(http_requests_total[5m]))
    ) > 0.05
  for: 5m
  labels:
    severity: critical
  annotations:
    summary: "Taxa de erro > 5%"
    action: "1. Verificar logs, 2. Rollback se deploy recente, 3. Escalar pods"

# WARNING: Potencial problema, investigar em hor√°rio comercial
- alert: HighLatency
  expr: |
    histogram_quantile(0.99, 
      rate(http_server_requests_seconds_bucket[5m])
    ) > 0.5
  for: 10m
  labels:
    severity: warning
  annotations:
    summary: "Lat√™ncia p99 > 500ms"
    action: "1. Verificar dashboard, 2. Analisar slow queries, 3. Considerar cache"

# INFO: Informacional, sem a√ß√£o imediata
- alert: LowTraffic
  expr: |
    rate(http_requests_total[5m]) < 10
  for: 15m
  labels:
    severity: info
  annotations:
    summary: "Tr√°fego baixo (< 10 RPS)"
    action: "Verificar se √© hor√°rio off-peak normal"
```

---

## 5. Prioriza√ß√£o

### 5.1 Matriz de Prioridade

```java
/**
 * MATRIZ DE PRIORIDADE DE ALERTAS
 *
 * Dimens√µes:
 * - Impacto no usu√°rio (baixo / m√©dio / alto)
 * - Urg√™ncia (pode esperar / horas / imediato)
 */

record AlertPriority(
    String name,
    Impact impact,
    Urgency urgency,
    String action
) {
    enum Impact {
        LOW,      // Usu√°rio n√£o afetado
        MEDIUM,   // Degrada√ß√£o de performance
        HIGH      // Usu√°rio n√£o consegue usar
    }

    enum Urgency {
        LOW,      // Pode esperar at√© pr√≥ximo dia √∫til
        MEDIUM,   // Resolver em algumas horas
        HIGH      // A√ß√£o imediata (< 15min)
    }

    /**
     * P0: Impact HIGH + Urgency HIGH
     *     Exemplo: API retornando 500 para todos
     *     A√ß√£o: Acordar on-call, rollback imediato
     *
     * P1: Impact HIGH + Urgency MEDIUM ou Impact MEDIUM + Urgency HIGH
     *     Exemplo: Lat√™ncia p99 > 2s (lentid√£o severa)
     *     A√ß√£o: Investigar em at√© 1h
     *
     * P2: Impact MEDIUM + Urgency MEDIUM
     *     Exemplo: CPU 80% (pr√≥ximo de satura√ß√£o)
     *     A√ß√£o: Ticket para escalar pods
     *
     * P3: Impact LOW ou Urgency LOW
     *     Exemplo: Disk 60% (muito buffer ainda)
     *     A√ß√£o: Monitorar, planejar limpeza
     */
}
```

---

### 5.2 On-Call Playbook

```java
/**
 * RUNBOOK: Passo a passo para cada alerta
 */

/**
 * Alerta: HighErrorRate (P0)
 *
 * 1. TRIAGE (5min):
 *    - Verificar dashboard: Qual endpoint?
 *    - Verificar logs: Qual exception?
 *    - Verificar deploy recente (√∫ltimas 2h)
 *
 * 2. MITIGATE (10min):
 *    - Se deploy recente: Rollback
 *    - Se depend√™ncia down: Ativar circuit breaker
 *    - Se overload: Escalar pods (kubectl scale deployment app --replicas=10)
 *
 * 3. RESOLVE (30min):
 *    - Identificar root cause
 *    - Deploy com fix
 *    - Validar error rate < 1%
 *
 * 4. POST-MORTEM (48h):
 *    - Documento de incident
 *    - 5 Whys para root cause
 *    - Action items para prevenir
 */

/**
 * Alerta: HighLatency (P1)
 *
 * 1. TRIAGE:
 *    - Dashboard: Qual endpoint lento?
 *    - APM traces: Qual opera√ß√£o demorando?
 *    - Database: Slow queries (pg_stat_statements)
 *
 * 2. MITIGATE:
 *    - Se query lenta: Criar √≠ndice tempor√°rio
 *    - Se N+1: Ativar cache
 *    - Se traffic spike: Ativar rate limiting
 *
 * 3. RESOLVE:
 *    - Otimizar query
 *    - Deploy com fix
 *    - Validar latency p99 < 500ms
 */
```

---

## üìä Checklist de Qualidade

- [ ] SLIs definidos para todos servi√ßos cr√≠ticos (latency, errors, saturation)
- [ ] SLOs com threshold baseado em requisito de neg√≥cio
- [ ] Error budget calculado e dashboard vis√≠vel
- [ ] Golden Signals monitorados (latency, traffic, errors, saturation)
- [ ] Alertas classificados (critical/warning/info)
- [ ] Runbooks para alertas P0 e P1
- [ ] On-call rotation configurada (PagerDuty/Opsgenie)
- [ ] Post-mortem process definido
- [ ] Alert fatigue < 10% (falsos positivos baixos)
- [ ] MTTD < 5min, MTTR < 30min (critical alerts)

---

## üéØ Exerc√≠cios Pr√°ticos

1. **B√°sico**: Definir SLI/SLO para API REST (latency, error rate)
2. **Intermedi√°rio**: Calcular error budget e burn rate
3. **Avan√ßado**: Criar alertas com m√∫ltiplas janelas (burn rate alerts)

---

## üìö Refer√™ncias

- [Google SRE Book - SLOs](https://sre.google/sre-book/service-level-objectives/)
- [Google SRE Workbook - Alerting on SLOs](https://sre.google/workbook/alerting-on-slos/)
- [Four Golden Signals](https://sre.google/sre-book/monitoring-distributed-systems/#xref_monitoring_golden-signals)
- [Prometheus Alerting Best Practices](https://prometheus.io/docs/practices/alerting/)

---

**Pr√≥ximo:** [13.2 - Prometheus Alerting](13.2-prometheus-alerting.md)
